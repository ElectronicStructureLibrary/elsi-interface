#if 0
!    This file is part of ELPA.
!
!    The ELPA library was originally created by the ELPA consortium,
!    consisting of the following organizations:
!
!    - Max Planck Computing and Data Facility (MPCDF), formerly known as
!      Rechenzentrum Garching der Max-Planck-Gesellschaft (RZG),
!    - Bergische Universität Wuppertal, Lehrstuhl für angewandte
!      Informatik,
!    - Technische Universität München, Lehrstuhl für Informatik mit
!      Schwerpunkt Wissenschaftliches Rechnen ,
!    - Fritz-Haber-Institut, Berlin, Abt. Theorie,
!    - Max-Plack-Institut für Mathematik in den Naturwissenschaften,
!      Leipzig, Abt. Komplexe Strukutren in Biologie und Kognition,
!      and
!    - IBM Deutschland GmbH
!
!    This particular source code file contains additions, changes and
!    enhancements authored by Intel Corporation which is not part of
!    the ELPA consortium.
!
!    More information can be found here:
!    http://elpa.mpcdf.mpg.de/
!
!    ELPA is free software: you can redistribute it and/or modify
!    it under the terms of the version 3 of the license of the
!    GNU Lesser General Public License as published by the Free
!    Software Foundation.
!
!    ELPA is distributed in the hope that it will be useful,
!    but WITHOUT ANY WARRANTY; without even the implied warranty of
!    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
!    GNU Lesser General Public License for more details.
!
!    You should have received a copy of the GNU Lesser General Public License
!    along with ELPA.  If not, see <http://www.gnu.org/licenses/>
!
!    ELPA reflects a substantial effort on the part of the original
!    ELPA consortium, and we ask you to respect the spirit of the
!    license that we chose: i.e., please contribute any changes you
!    may have back to the original ELPA library distribution, and keep
!    any derivatives of ELPA under the same license that we chose for
!    the original distribution, the GNU Lesser General Public License.
!
!
! ELPA1 -- Faster replacements for ScaLAPACK symmetric eigenvalue routines
!
! Copyright of the original code rests with the authors inside the ELPA
! consortium. The copyright of any additional modifications shall rest
! with their original authors, but shall adhere to the licensing terms
! distributed along with the original code in the file "COPYING".
#endif

#ifdef DOUBLE_PRECISION_COMPLEX
    subroutine tridiag_complex_double(na, a, lda, nblk, matrixCols, mpi_comm_rows, mpi_comm_cols, d, e, tau)
#else
    subroutine tridiag_complex_single(na, a, lda, nblk, matrixCols, mpi_comm_rows, mpi_comm_cols, d, e, tau)
#endif
    !-------------------------------------------------------------------------------
    !  tridiag_complex: Reduces a distributed hermitian matrix to tridiagonal form
    !                   (like Scalapack Routine PZHETRD)
    !
    !  Parameters
    !
    !  na          Order of matrix
    !
    !  a(lda,matrixCols)    Distributed matrix which should be reduced.
    !              Distribution is like in Scalapack.
    !              Opposed to PZHETRD, a(:,:) must be set completely (upper and lower half)
    !              a(:,:) is overwritten on exit with the Householder vectors
    !
    !  lda         Leading dimension of a
    !  matrixCols  local columns of matrix a
    !
    !  nblk        blocksize of cyclic distribution, must be the same in both directions!
    !
    !  mpi_comm_rows
    !  mpi_comm_cols
    !              MPI-Communicators for rows/columns
    !
    !  d(na)       Diagonal elements (returned), identical on all processors
    !
    !  e(na)       Off-Diagonal elements (returned), identical on all processors
    !
    !  tau(na)     Factors for the Householder vectors (returned), needed for back transformation
    !
    !-------------------------------------------------------------------------------
#ifdef HAVE_DETAILED_TIMINGS
      use timings
#endif
      use precision
      implicit none

      integer(kind=ik)              :: na, lda, nblk, matrixCols, mpi_comm_rows, mpi_comm_cols
      complex(kind=COMPLEX_DATATYPE)              :: tau(na)
#ifdef DESPERATELY_WANT_ASSUMED_SIZE
      complex(kind=COMPLEX_DATATYPE)              :: a(lda,*)
#else
      complex(kind=COMPLEX_DATATYPE)              :: a(lda,matrixCols)
#endif
      real(kind=REAL_DATATYPE)                 :: d(na), e(na)

      integer(kind=ik), parameter   :: max_stored_rows = 32
#ifdef DOUBLE_PRECISION_COMPLEX
      complex(kind=ck8), parameter   :: CZERO = (0.0_rk8,0.0_rk8), CONE = (1.0_rk8,0.0_rk8)
#else
      complex(kind=ck4), parameter   :: CZERO = (0.0_rk4,0.0_rk4), CONE = (1.0_rk4,0.0_rk4)
#endif
      integer(kind=ik)              :: my_prow, my_pcol, np_rows, np_cols, mpierr
      integer(kind=ik)              :: totalblocks, max_blocks_row, max_blocks_col, max_local_rows, max_local_cols
      integer(kind=ik)              :: l_cols, l_rows, nstor
      integer(kind=ik)              :: istep, i, j, lcs, lce, lrs, lre
      integer(kind=ik)              :: tile_size, l_rows_tile, l_cols_tile

#ifdef WITH_OPENMP
      integer(kind=ik)              :: my_thread, n_threads, max_threads, n_iter
      integer(kind=ik)              :: omp_get_thread_num, omp_get_num_threads, omp_get_max_threads
#endif

      real(kind=REAL_DATATYPE)                 :: vnorm2
      complex(kind=COMPLEX_DATATYPE)              :: vav, xc, aux(2*max_stored_rows),  aux1(2), aux2(2), vrl, xf

      complex(kind=COMPLEX_DATATYPE), allocatable :: tmp(:), vr(:), vc(:), ur(:), uc(:), vur(:,:), uvc(:,:)
#ifdef WITH_OPENMP
      complex(kind=COMPLEX_DATATYPE), allocatable :: ur_p(:,:), uc_p(:,:)
#endif
      real(kind=REAL_DATATYPE), allocatable    :: tmpr(:)
      integer(kind=ik)              :: istat
      character(200)                :: errorMessage

#ifdef HAVE_DETAILED_TIMINGS
#ifdef DOUBLE_PRECISION_COMPLEX
      call timer%start("tridiag_complex_double")
#else
      call timer%start("tridiag_complex_single")
#endif
#endif

      call mpi_comm_rank(mpi_comm_rows,my_prow,mpierr)
      call mpi_comm_size(mpi_comm_rows,np_rows,mpierr)
      call mpi_comm_rank(mpi_comm_cols,my_pcol,mpierr)
      call mpi_comm_size(mpi_comm_cols,np_cols,mpierr)

      ! Matrix is split into tiles; work is done only for tiles on the diagonal or above

      tile_size = nblk*least_common_multiple(np_rows,np_cols) ! minimum global tile size
      tile_size = ((128*max(np_rows,np_cols)-1)/tile_size+1)*tile_size ! make local tiles at least 128 wide

      l_rows_tile = tile_size/np_rows ! local rows of a tile
      l_cols_tile = tile_size/np_cols ! local cols of a tile


      totalblocks = (na-1)/nblk + 1
      max_blocks_row = (totalblocks-1)/np_rows + 1
      max_blocks_col = (totalblocks-1)/np_cols + 1

      max_local_rows = max_blocks_row*nblk
      max_local_cols = max_blocks_col*nblk

      allocate(tmp(MAX(max_local_rows,max_local_cols)), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when allocating tmp "//errorMessage
       stop
      endif

      allocate(vr(max_local_rows+1), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when allocating vr "//errorMessage
       stop
      endif

      allocate(ur(max_local_rows), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when allocating ur "//errorMessage
       stop
      endif

      allocate(vc(max_local_cols), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when allocating vc "//errorMessage
       stop
      endif

      allocate(uc(max_local_cols), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when allocating uc "//errorMessage
       stop
      endif

#ifdef WITH_OPENMP
      max_threads = omp_get_max_threads()

      allocate(ur_p(max_local_rows,0:max_threads-1), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when allocating ur_p "//errorMessage
       stop
      endif

      allocate(uc_p(max_local_cols,0:max_threads-1), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when allocating uc_p "//errorMessage
       stop
      endif
#endif

      tmp = 0
      vr = 0
      ur = 0
      vc = 0
      uc = 0

      allocate(vur(max_local_rows,2*max_stored_rows), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when allocating vur "//errorMessage
       stop
      endif

      allocate(uvc(max_local_cols,2*max_stored_rows), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when allocating uvc "//errorMessage
       stop
      endif

      d(:) = 0
      e(:) = 0
      tau(:) = 0

      nstor = 0

      l_rows = local_index(na, my_prow, np_rows, nblk, -1) ! Local rows of a
      l_cols = local_index(na, my_pcol, np_cols, nblk, -1) ! Local cols of a
      if (my_prow==prow(na, nblk, np_rows) .and. my_pcol==pcol(na, nblk, np_cols)) d(na) = a(l_rows,l_cols)

      do istep=na,3,-1

        ! Calculate number of local rows and columns of the still remaining matrix
        ! on the local processor

        l_rows = local_index(istep-1, my_prow, np_rows, nblk, -1)
        l_cols = local_index(istep-1, my_pcol, np_cols, nblk, -1)

        ! Calculate vector for Householder transformation on all procs
        ! owning column istep

        if (my_pcol==pcol(istep, nblk, np_cols)) then

          ! Get vector to be transformed; distribute last element and norm of
          ! remaining elements to all procs in current column

          vr(1:l_rows) = a(1:l_rows,l_cols+1)
          if (nstor>0 .and. l_rows>0) then
            aux(1:2*nstor) = conjg(uvc(l_cols+1,1:2*nstor))
#ifdef DOUBLE_PRECISION_COMPLEX
            call ZGEMV('N', l_rows, 2*nstor, CONE, vur, ubound(vur,dim=1), &
                        aux, 1, CONE, vr, 1)
#else
            call CGEMV('N', l_rows, 2*nstor, CONE, vur, ubound(vur,dim=1), &
                        aux, 1, CONE, vr, 1)
#endif
          endif

          if (my_prow==prow(istep-1, nblk, np_rows)) then
            aux1(1) = dot_product(vr(1:l_rows-1),vr(1:l_rows-1))
            aux1(2) = vr(l_rows)
          else
            aux1(1) = dot_product(vr(1:l_rows),vr(1:l_rows))
            aux1(2) = 0.
          endif
#ifdef WITH_MPI

#ifdef DOUBLE_PRECISION_COMPLEX
          call mpi_allreduce(aux1, aux2, 2, MPI_DOUBLE_COMPLEX, MPI_SUM, mpi_comm_rows, mpierr)
#else
          call mpi_allreduce(aux1, aux2, 2, MPI_COMPLEX, MPI_SUM, mpi_comm_rows, mpierr)
#endif

          vnorm2 = aux2(1)
          vrl    = aux2(2)

#else /* WITH_MPI */
!          aux2 = aux1

          vnorm2 = aux1(1)
          vrl    = aux1(2)

#endif /* WITH_MPI */

!          vnorm2 = aux2(1)
!          vrl    = aux2(2)

          ! Householder transformation
#ifdef DOUBLE_PRECISION_COMPLEX
          call hh_transform_complex_double(vrl, vnorm2, xf, tau(istep))
#else
          call hh_transform_complex_single(vrl, vnorm2, xf, tau(istep))
#endif
          ! Scale vr and store Householder vector for back transformation

          vr(1:l_rows) = vr(1:l_rows) * xf
          if (my_prow==prow(istep-1, nblk, np_rows)) then
            vr(l_rows) = 1.
            e(istep-1) = vrl
          endif
          a(1:l_rows,l_cols+1) = vr(1:l_rows) ! store Householder vector for back transformation

        endif

        ! Broadcast the Householder vector (and tau) along columns

        if (my_pcol==pcol(istep, nblk, np_cols)) vr(l_rows+1) = tau(istep)
#ifdef WITH_MPI

#ifdef DOUBLE_PRECISION_COMPLEX
        call MPI_Bcast(vr, l_rows+1, MPI_DOUBLE_COMPLEX, pcol(istep, nblk, np_cols), mpi_comm_cols, mpierr)
#else
        call MPI_Bcast(vr, l_rows+1, MPI_COMPLEX, pcol(istep, nblk, np_cols), mpi_comm_cols, mpierr)
#endif

#endif /* WITH_MPI */
        tau(istep) =  vr(l_rows+1)

        ! Transpose Householder vector vr -> vc

!        call elpa_transpose_vectors  (vr, 2*ubound(vr,dim=1), mpi_comm_rows, &
!                                      vc, 2*ubound(vc,dim=1), mpi_comm_cols, &
!                                      1, 2*(istep-1), 1, 2*nblk)
#ifdef DOUBLE_PRECISION_COMPLEX
        call elpa_transpose_vectors_complex_double  (vr, ubound(vr,dim=1), mpi_comm_rows, &
                                              vc, ubound(vc,dim=1), mpi_comm_cols, &
                                              1, (istep-1), 1, nblk)
#else
        call elpa_transpose_vectors_complex_single  (vr, ubound(vr,dim=1), mpi_comm_rows, &
                                              vc, ubound(vc,dim=1), mpi_comm_cols, &
                                              1, (istep-1), 1, nblk)
#endif
        ! Calculate u = (A + VU**T + UV**T)*v

        ! For cache efficiency, we use only the upper half of the matrix tiles for this,
        ! thus the result is partly in uc(:) and partly in ur(:)

        uc(1:l_cols) = 0
        ur(1:l_rows) = 0
        if (l_rows>0 .and. l_cols>0) then

#ifdef WITH_OPENMP

#ifdef HAVE_DETAILED_TIMINGS
#ifdef DOUBLE_PRECISION_COMPLEX
          call timer%start("OpenMP parallel_double")
#else
          call timer%start("OpenMP parallel_single")
#endif
#endif

!$OMP PARALLEL PRIVATE(my_thread,n_threads,n_iter,i,lcs,lce,j,lrs,lre)

          my_thread = omp_get_thread_num()
          n_threads = omp_get_num_threads()

          n_iter = 0

          uc_p(1:l_cols,my_thread) = 0.
          ur_p(1:l_rows,my_thread) = 0.
#endif

          do i=0,(istep-2)/tile_size
            lcs = i*l_cols_tile+1
            lce = min(l_cols,(i+1)*l_cols_tile)
            if (lce<lcs) cycle
            do j=0,i
              lrs = j*l_rows_tile+1
              lre = min(l_rows,(j+1)*l_rows_tile)
              if (lre<lrs) cycle
#ifdef WITH_OPENMP
              if (mod(n_iter,n_threads) == my_thread) then
#ifdef DOUBLE_PRECISION_COMPLEX
                call ZGEMV('C', lre-lrs+1 ,lce-lcs+1, CONE, a(lrs,lcs), lda, vr(lrs), 1, CONE, uc_p(lcs,my_thread), 1)
                if (i/=j) then
                  call ZGEMV('N', lre-lrs+1, lce-lcs+1, CONE, a(lrs,lcs), lda, vc(lcs), 1, CONE, ur_p(lrs,my_thread), 1)
                endif
#else
                call CGEMV('C', lre-lrs+1 ,lce-lcs+1, CONE, a(lrs,lcs), lda, vr(lrs), 1, CONE, uc_p(lcs,my_thread), 1)
                if (i/=j) then
                  call CGEMV('N', lre-lrs+1, lce-lcs+1, CONE, a(lrs,lcs), lda, vc(lcs), 1, CONE, ur_p(lrs,my_thread), 1)
                endif
#endif
              endif
              n_iter = n_iter+1
#else /* WITH_OPENMP */

#ifdef DOUBLE_PRECISION_COMPLEX
              call ZGEMV('C', lre-lrs+1, lce-lcs+1, CONE, a(lrs,lcs), lda, vr(lrs), 1, CONE, uc(lcs), 1)
              if (i/=j) then
                call ZGEMV('N', lre-lrs+1, lce-lcs+1, CONE, a(lrs,lcs), lda, vc(lcs), 1, CONE, ur(lrs), 1)
              endif
#else
              call CGEMV('C', lre-lrs+1, lce-lcs+1, CONE, a(lrs,lcs), lda, vr(lrs), 1, CONE, uc(lcs), 1)
              if (i/=j) then
                call CGEMV('N', lre-lrs+1, lce-lcs+1, CONE, a(lrs,lcs), lda, vc(lcs), 1, CONE, ur(lrs), 1)
              endif
#endif

#endif /* WITH_OPENMP */
            enddo
          enddo

#ifdef WITH_OPENMP
!$OMP END PARALLEL
#ifdef HAVE_DETAILED_TIMINGS
#ifdef DOUBLE_PRECISION_COMPLEX
          call timer%stop("OpenMP parallel_double")
#else
          call timer%stop("OpenMP parallel_single")
#endif
#endif

          do i=0,max_threads-1
            uc(1:l_cols) = uc(1:l_cols) + uc_p(1:l_cols,i)
            ur(1:l_rows) = ur(1:l_rows) + ur_p(1:l_rows,i)
          enddo
#endif

          if (nstor>0) then
#ifdef DOUBLE_PRECISION_COMPLEX
            call ZGEMV('C', l_rows, 2*nstor, CONE, vur, ubound(vur,dim=1), vr,  1, CZERO, aux, 1)
            call ZGEMV('N', l_cols, 2*nstor, CONE, uvc, ubound(uvc,dim=1), aux, 1, CONE, uc, 1)
#else
            call CGEMV('C', l_rows, 2*nstor, CONE, vur, ubound(vur,dim=1), vr,  1, CZERO, aux, 1)
            call CGEMV('N', l_cols, 2*nstor, CONE, uvc, ubound(uvc,dim=1), aux, 1, CONE, uc, 1)
#endif
          endif

        endif

        ! Sum up all ur(:) parts along rows and add them to the uc(:) parts
        ! on the processors containing the diagonal
        ! This is only necessary if ur has been calculated, i.e. if the
        ! global tile size is smaller than the global remaining matrix

        if (tile_size < istep-1) then
#ifdef DOUBLE_PRECISION_COMPLEX
          call elpa_reduce_add_vectors_COMPLEX_double  (ur, ubound(ur,dim=1), mpi_comm_rows, &
                                          uc, ubound(uc,dim=1), mpi_comm_cols, &
                                          (istep-1), 1, nblk)
#else
          call elpa_reduce_add_vectors_COMPLEX_single  (ur, ubound(ur,dim=1), mpi_comm_rows, &
                                          uc, ubound(uc,dim=1), mpi_comm_cols, &
                                          (istep-1), 1, nblk)
#endif
        endif

        ! Sum up all the uc(:) parts, transpose uc -> ur

        if (l_cols>0) then
          tmp(1:l_cols) = uc(1:l_cols)
#ifdef WITH_MPI

#ifdef DOUBLE_PRECISION_COMPLEX
          call mpi_allreduce(tmp, uc, l_cols, MPI_DOUBLE_COMPLEX, MPI_SUM, mpi_comm_rows, mpierr)
#else
          call mpi_allreduce(tmp, uc, l_cols, MPI_COMPLEX, MPI_SUM, mpi_comm_rows, mpierr)
#endif

#else /* WITH_MPI */
          uc = tmp
#endif /* WITH_MPI */
        endif

!        call elpa_transpose_vectors  (uc, 2*ubound(uc,dim=1), mpi_comm_cols, &
!                                      ur, 2*ubound(ur,dim=1), mpi_comm_rows, &
!                                      1, 2*(istep-1), 1, 2*nblk)
#ifdef DOUBLE_PRECISION_COMPLEX
        call elpa_transpose_vectors_complex_double  (uc, ubound(uc,dim=1), mpi_comm_cols, &
                                              ur, ubound(ur,dim=1), mpi_comm_rows, &
                                              1, (istep-1), 1, nblk)
#else
        call elpa_transpose_vectors_complex_single  (uc, ubound(uc,dim=1), mpi_comm_cols, &
                                              ur, ubound(ur,dim=1), mpi_comm_rows, &
                                              1, (istep-1), 1, nblk)
#endif


        ! calculate u**T * v (same as v**T * (A + VU**T + UV**T) * v )

        xc = 0
        if (l_cols>0) xc = dot_product(vc(1:l_cols),uc(1:l_cols))
#ifdef WITH_MPI

#ifdef DOUBLE_PRECISION_COMPLEX
        call mpi_allreduce(xc, vav, 1 , MPI_DOUBLE_COMPLEX, MPI_SUM, mpi_comm_cols, mpierr)
#else
        call mpi_allreduce(xc, vav, 1 , MPI_COMPLEX, MPI_SUM, mpi_comm_cols, mpierr)
#endif

#else /* WITH_MPI */
        vav = xc
#endif /* WITH_MPI */

        ! store u and v in the matrices U and V
        ! these matrices are stored combined in one here

        do j=1,l_rows
          vur(j,2*nstor+1) = conjg(tau(istep))*vr(j)
          vur(j,2*nstor+2) = 0.5*conjg(tau(istep))*vav*vr(j) - ur(j)
        enddo
        do j=1,l_cols
          uvc(j,2*nstor+1) = 0.5*conjg(tau(istep))*vav*vc(j) - uc(j)
          uvc(j,2*nstor+2) = conjg(tau(istep))*vc(j)
        enddo

        nstor = nstor+1

        ! If the limit of max_stored_rows is reached, calculate A + VU**T + UV**T

        if (nstor==max_stored_rows .or. istep==3) then

          do i=0,(istep-2)/tile_size
            lcs = i*l_cols_tile+1
            lce = min(l_cols,(i+1)*l_cols_tile)
            lrs = 1
            lre = min(l_rows,(i+1)*l_rows_tile)
            if (lce<lcs .or. lre<lrs) cycle
#ifdef DOUBLE_PRECISION_COMPLEX
            call ZGEMM('N', 'C', lre-lrs+1, lce-lcs+1, 2*nstor, CONE, &
                         vur(lrs,1), ubound(vur,dim=1), uvc(lcs,1), ubound(uvc,dim=1), &
                         CONE, a(lrs,lcs), lda)
#else
            call CGEMM('N', 'C', lre-lrs+1, lce-lcs+1, 2*nstor, CONE, &
                         vur(lrs,1), ubound(vur,dim=1), uvc(lcs,1), ubound(uvc,dim=1), &
                         CONE, a(lrs,lcs), lda)
#endif
          enddo

          nstor = 0

        endif

        if (my_prow==prow(istep-1, nblk, np_rows) .and. my_pcol==pcol(istep-1, nblk, np_cols)) then
          if (nstor>0) a(l_rows,l_cols) = a(l_rows,l_cols) &
                          + dot_product(vur(l_rows,1:2*nstor),uvc(l_cols,1:2*nstor))
          d(istep-1) = a(l_rows,l_cols)
        endif

      enddo ! istep

      ! Store e(1) and d(1)

      if (my_pcol==pcol(2, nblk, np_cols)) then
        if (my_prow==prow(1, nblk, np_rows)) then
          ! We use last l_cols value of loop above
          vrl = a(1,l_cols)
#ifdef DOUBLE_PRECISION_COMPLEX
          call hh_transform_complex_double(vrl, 0.0_rk8, xf, tau(2))
#else
          call hh_transform_complex_single(vrl, 0.0_rk4, xf, tau(2))
#endif
          e(1) = vrl
          a(1,l_cols) = 1. ! for consistency only
        endif

#ifdef WITH_MPI

#ifdef DOUBLE_PRECISION_COMPLEX
        call mpi_bcast(tau(2), 1, MPI_DOUBLE_COMPLEX, prow(1, nblk, np_rows), mpi_comm_rows, mpierr)
#else
        call mpi_bcast(tau(2), 1, MPI_COMPLEX, prow(1, nblk, np_rows), mpi_comm_rows, mpierr)
#endif

#endif /* WITH_MPI */
      endif

#ifdef WITH_MPI

#ifdef DOUBLE_PRECISION_COMPLEX
      call mpi_bcast(tau(2), 1, MPI_DOUBLE_COMPLEX, pcol(2, nblk, np_cols), mpi_comm_cols, mpierr)
#else
      call mpi_bcast(tau(2), 1, MPI_COMPLEX, pcol(2, nblk, np_cols), mpi_comm_cols, mpierr)
#endif

#endif /* WITH_MPI */


      if (my_prow==prow(1, nblk, np_rows) .and. my_pcol==pcol(1, nblk, np_cols)) d(1) = a(1,1)

      deallocate(tmp, vr, ur, vc, uc, vur, uvc, stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when deallocating tmp "//errorMessage
       stop
      endif
      ! distribute the arrays d and e to all processors

      allocate(tmpr(na), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when allocating tmpr "//errorMessage
       stop
      endif

#ifdef WITH_MPI

#ifdef DOUBLE_PRECISION_COMPLEX
      tmpr = d
      call mpi_allreduce(tmpr, d, na, MPI_REAL8, MPI_SUM, mpi_comm_rows, mpierr)
      tmpr = d
      call mpi_allreduce(tmpr, d, na, MPI_REAL8 ,MPI_SUM, mpi_comm_cols, mpierr)
      tmpr = e
      call mpi_allreduce(tmpr, e, na, MPI_REAL8, MPI_SUM, mpi_comm_rows, mpierr)
      tmpr = e
      call mpi_allreduce(tmpr, e, na, MPI_REAL8, MPI_SUM, mpi_comm_cols, mpierr)
#else
      tmpr = d
      call mpi_allreduce(tmpr, d, na, MPI_REAL4, MPI_SUM, mpi_comm_rows, mpierr)
      tmpr = d
      call mpi_allreduce(tmpr, d, na, MPI_REAL4 ,MPI_SUM, mpi_comm_cols, mpierr)
      tmpr = e
      call mpi_allreduce(tmpr, e, na, MPI_REAL4, MPI_SUM, mpi_comm_rows, mpierr)
      tmpr = e
      call mpi_allreduce(tmpr, e, na, MPI_REAL4, MPI_SUM, mpi_comm_cols, mpierr)
#endif

#endif /* WITH_MPI */
      deallocate(tmpr, stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"tridiag_complex: error when deallocating tmpr "//errorMessage
       stop
      endif

#ifdef HAVE_DETAILED_TIMINGS
#ifdef DOUBLE_PRECISION_COMPLEX
      call timer%stop("tridiag_complex_double")
#else
      call timer%stop("tridiag_complex_single")
#endif
#endif

#ifdef DOUBLE_PRECISION_COMPLEX
    end subroutine tridiag_complex_double
#else
    end subroutine tridiag_complex_single
#endif

#ifdef DOUBLE_PRECISION_COMPLEX
    subroutine trans_ev_complex_double(na, nqc, a, lda, tau, q, ldq, nblk, matrixCols, mpi_comm_rows, mpi_comm_cols)
#else
    subroutine trans_ev_complex_single(na, nqc, a, lda, tau, q, ldq, nblk, matrixCols, mpi_comm_rows, mpi_comm_cols)
#endif
    !-------------------------------------------------------------------------------
    !  trans_ev_complex: Transforms the eigenvectors of a tridiagonal matrix back
    !                    to the eigenvectors of the original matrix
    !                    (like Scalapack Routine PZUNMTR)
    !
    !  Parameters
    !
    !  na          Order of matrix a, number of rows of matrix q
    !
    !  nqc         Number of columns of matrix q
    !
    !  a(lda,matrixCols)    Matrix containing the Householder vectors (i.e. matrix a after tridiag_complex)
    !              Distribution is like in Scalapack.
    !
    !  lda         Leading dimension of a
    !
    !  tau(na)     Factors of the Householder vectors
    !
    !  q           On input: Eigenvectors of tridiagonal matrix
    !              On output: Transformed eigenvectors
    !              Distribution is like in Scalapack.
    !
    !  ldq         Leading dimension of q
    !
    !  nblk        blocksize of cyclic distribution, must be the same in both directions!
    !
    !  mpi_comm_rows
    !  mpi_comm_cols
    !              MPI-Communicators for rows/columns
    !
    !-------------------------------------------------------------------------------
#ifdef HAVE_DETAILED_TIMINGS
      use timings
#endif
      use precision
      implicit none

      integer(kind=ik)              ::  na, nqc, lda, ldq, nblk, matrixCols, mpi_comm_rows, mpi_comm_cols
      complex(kind=COMPLEX_DATATYPE)              ::  tau(na)
#ifdef DESPERATELY_WANT_ASSUMED_SIZE
      complex(kind=COMPLEX_DATATYPE)              :: a(lda,*), q(ldq,*)
#else
      complex(kind=COMPLEX_DATATYPE)              ::  a(lda,matrixCols), q(ldq,matrixCols)
#endif
      integer(kind=ik)              :: max_stored_rows
#ifdef DOUBLE_PRECISION_COMPLEX
      complex(kind=ck8), parameter   :: CZERO = (0.0_rk8,0.0_rk8), CONE = (1.0_rk8,0.0_rk8)
#else
      complex(kind=ck4), parameter   :: CZERO = (0.0_rk4,0.0_rk4), CONE = (1.0_rk4,0.0_rk4)
#endif
      integer(kind=ik)              :: my_prow, my_pcol, np_rows, np_cols, mpierr
      integer(kind=ik)              :: totalblocks, max_blocks_row, max_blocks_col, max_local_rows, max_local_cols
      integer(kind=ik)              :: l_cols, l_rows, l_colh, nstor
      integer(kind=ik)              :: istep, i, n, nc, ic, ics, ice, nb, cur_pcol

      complex(kind=COMPLEX_DATATYPE), allocatable :: tmp1(:), tmp2(:), hvb(:), hvm(:,:)
      complex(kind=COMPLEX_DATATYPE), allocatable :: tmat(:,:), h1(:), h2(:)
      integer(kind=ik)              :: istat
      character(200)                :: errorMessage
#ifdef HAVE_DETAILED_TIMINGS
#ifdef DOUBLE_PRECISION_COMPLEX
      call timer%start("trans_ev_complex_double")
#else
      call timer%start("trans_ev_complex_single")
#endif
#endif

      call mpi_comm_rank(mpi_comm_rows,my_prow,mpierr)
      call mpi_comm_size(mpi_comm_rows,np_rows,mpierr)
      call mpi_comm_rank(mpi_comm_cols,my_pcol,mpierr)
      call mpi_comm_size(mpi_comm_cols,np_cols,mpierr)

      totalblocks = (na-1)/nblk + 1
      max_blocks_row = (totalblocks-1)/np_rows + 1
      max_blocks_col = ((nqc-1)/nblk)/np_cols + 1  ! Columns of q!

      max_local_rows = max_blocks_row*nblk
      max_local_cols = max_blocks_col*nblk

      max_stored_rows = (63/nblk+1)*nblk

      allocate(tmat(max_stored_rows,max_stored_rows), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"trans_ev_complex: error when allocating tmat "//errorMessage
       stop
      endif

      allocate(h1(max_stored_rows*max_stored_rows), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"trans_ev_complex: error when allocating h1 "//errorMessage
       stop
      endif

      allocate(h2(max_stored_rows*max_stored_rows), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"trans_ev_complex: error when allocating h2 "//errorMessage
       stop
      endif

      allocate(tmp1(max_local_cols*max_stored_rows), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"trans_ev_complex: error when allocating tmp1 "//errorMessage
       stop
      endif

      allocate(tmp2(max_local_cols*max_stored_rows), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"trans_ev_complex: error when allocating tmp2 "//errorMessage
       stop
      endif

      allocate(hvb(max_local_rows*nblk), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"trans_ev_complex: error when allocating hvb "//errorMessage
       stop
      endif

      allocate(hvm(max_local_rows,max_stored_rows), stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"trans_ev_complex: error when allocating hvm "//errorMessage
       stop
      endif

      hvm = 0   ! Must be set to 0 !!!
      hvb = 0   ! Safety only

      l_cols = local_index(nqc, my_pcol, np_cols, nblk, -1) ! Local columns of q

      nstor = 0

      ! In the complex case tau(2) /= 0
      if (my_prow == prow(1, nblk, np_rows)) then
#ifdef DOUBLE_PRECISION_COMPLEX
        q(1,1:l_cols) = q(1,1:l_cols)*((1.0_rk8,0.0_rk8)-tau(2))
#else
        q(1,1:l_cols) = q(1,1:l_cols)*((1.0_rk4,0.0_rk4)-tau(2))
#endif
      endif

      do istep=1,na,nblk

        ics = MAX(istep,3)
        ice = MIN(istep+nblk-1,na)
        if (ice<ics) cycle

        cur_pcol = pcol(istep, nblk, np_cols)

        nb = 0
        do ic=ics,ice

          l_colh = local_index(ic  , my_pcol, np_cols, nblk, -1) ! Column of Householder vector
          l_rows = local_index(ic-1, my_prow, np_rows, nblk, -1) ! # rows of Householder vector


          if (my_pcol==cur_pcol) then
            hvb(nb+1:nb+l_rows) = a(1:l_rows,l_colh)
            if (my_prow==prow(ic-1, nblk, np_rows)) then
              hvb(nb+l_rows) = 1.
            endif
          endif

          nb = nb+l_rows
        enddo

#ifdef WITH_MPI
        if (nb>0) &
#ifdef DOUBLE_PRECISION_COMPLEX
           call MPI_Bcast(hvb, nb, MPI_DOUBLE_COMPLEX, cur_pcol, mpi_comm_cols, mpierr)
#else
           call MPI_Bcast(hvb, nb, MPI_COMPLEX, cur_pcol, mpi_comm_cols, mpierr)
#endif

#endif /* WITH_MPI */
        nb = 0
        do ic=ics,ice
          l_rows = local_index(ic-1, my_prow, np_rows, nblk, -1) ! # rows of Householder vector
          hvm(1:l_rows,nstor+1) = hvb(nb+1:nb+l_rows)
          nstor = nstor+1
          nb = nb+l_rows
        enddo

        ! Please note: for smaller matix sizes (na/np_rows<=256), a value of 32 for nstor is enough!
        if (nstor+nblk>max_stored_rows .or. istep+nblk>na .or. (na/np_rows<=256 .and. nstor>=32)) then

          ! Calculate scalar products of stored vectors.
          ! This can be done in different ways, we use zherk

          tmat = 0
          if (l_rows>0) &
#ifdef DOUBLE_PRECISION_COMPLEX
             call zherk('U', 'C', nstor, l_rows, CONE, hvm, ubound(hvm,dim=1), CZERO, tmat, max_stored_rows)
#else
             call cherk('U', 'C', nstor, l_rows, CONE, hvm, ubound(hvm,dim=1), CZERO, tmat, max_stored_rows)
#endif
          nc = 0
          do n=1,nstor-1
            h1(nc+1:nc+n) = tmat(1:n,n+1)
            nc = nc+n
          enddo
#ifdef WITH_MPI

#ifdef DOUBLE_PRECISION_COMPLEX
          if (nc>0) call mpi_allreduce(h1, h2, nc, MPI_DOUBLE_COMPLEX, MPI_SUM, mpi_comm_rows, mpierr)
#else
          if (nc>0) call mpi_allreduce(h1, h2, nc, MPI_COMPLEX, MPI_SUM, mpi_comm_rows, mpierr)
#endif

#else /* WITH_MPI */
          if (nc>0) h2=h1
#endif /* WITH_MPI */
          ! Calculate triangular matrix T

          nc = 0
          tmat(1,1) = tau(ice-nstor+1)
          do n=1,nstor-1
#ifdef DOUBLE_PRECISION_COMPLEX
            call ztrmv('L', 'C', 'N', n, tmat, max_stored_rows, h2(nc+1),1)
#else
            call ctrmv('L', 'C', 'N', n, tmat, max_stored_rows, h2(nc+1),1)
#endif
            tmat(n+1,1:n) = -conjg(h2(nc+1:nc+n))*tau(ice-nstor+n+1)
            tmat(n+1,n+1) = tau(ice-nstor+n+1)
            nc = nc+n
          enddo

          ! Q = Q - V * T * V**T * Q

          if (l_rows>0) then
#ifdef DOUBLE_PRECISION_COMPLEX
            call zgemm('C', 'N', nstor, l_cols, l_rows, CONE, hvm, ubound(hvm,dim=1), &
                        q, ldq, CZERO, tmp1 ,nstor)
#else
            call cgemm('C', 'N', nstor, l_cols, l_rows, CONE, hvm, ubound(hvm,dim=1), &
                        q, ldq, CZERO, tmp1 ,nstor)
#endif
          else
            tmp1(1:l_cols*nstor) = 0
          endif
#ifdef DOUBLE_PRECISION_COMPLEX

#ifdef WITH_MPI
          call mpi_allreduce(tmp1, tmp2, nstor*l_cols, MPI_DOUBLE_COMPLEX, MPI_SUM, mpi_comm_rows, mpierr)

          if (l_rows>0) then
            call ztrmm('L', 'L', 'N', 'N', nstor, l_cols, CONE, tmat, max_stored_rows, tmp2, nstor)
            call zgemm('N', 'N', l_rows, l_cols, nstor, -CONE, hvm, ubound(hvm,dim=1), &
                        tmp2, nstor, CONE, q, ldq)
          endif

#else
!          tmp2 = tmp1

          if (l_rows>0) then
            call ztrmm('L', 'L', 'N', 'N', nstor, l_cols, CONE, tmat, max_stored_rows, tmp1, nstor)
            call zgemm('N', 'N', l_rows, l_cols, nstor, -CONE, hvm, ubound(hvm,dim=1), &
                        tmp1, nstor, CONE, q, ldq)
          endif

#endif

!          if (l_rows>0) then
!            call ztrmm('L', 'L', 'N', 'N', nstor, l_cols, CONE, tmat, max_stored_rows, tmp2, nstor)
!            call zgemm('N', 'N', l_rows, l_cols, nstor, -CONE, hvm, ubound(hvm,dim=1), &
!                        tmp2, nstor, CONE, q, ldq)
!          endif

#else /* DOUBLE_PRECISION_COMPLEX */

#ifdef WITH_MPI
          call mpi_allreduce(tmp1, tmp2, nstor*l_cols, MPI_COMPLEX, MPI_SUM, mpi_comm_rows, mpierr)

          if (l_rows>0) then
            call ctrmm('L', 'L', 'N', 'N', nstor, l_cols, CONE, tmat, max_stored_rows, tmp2, nstor)
            call cgemm('N', 'N', l_rows, l_cols, nstor, -CONE, hvm, ubound(hvm,dim=1), &
                        tmp2, nstor, CONE, q, ldq)
          endif

#else
!          tmp2 = tmp1

          if (l_rows>0) then
            call ctrmm('L', 'L', 'N', 'N', nstor, l_cols, CONE, tmat, max_stored_rows, tmp1, nstor)
            call cgemm('N', 'N', l_rows, l_cols, nstor, -CONE, hvm, ubound(hvm,dim=1), &
                        tmp1, nstor, CONE, q, ldq)
          endif

#endif
!
!          if (l_rows>0) then
!            call ctrmm('L', 'L', 'N', 'N', nstor, l_cols, CONE, tmat, max_stored_rows, tmp2, nstor)
!            call cgemm('N', 'N', l_rows, l_cols, nstor, -CONE, hvm, ubound(hvm,dim=1), &
!                        tmp2, nstor, CONE, q, ldq)
!          endif

#endif /* DOUBLE_PRECISION_COMPLEX */
          nstor = 0
        endif

      enddo

      deallocate(tmat, h1, h2, tmp1, tmp2, hvb, hvm, stat=istat, errmsg=errorMessage)
      if (istat .ne. 0) then
       print *,"trans_ev_complex: error when deallocating hvb "//errorMessage
       stop
      endif

#ifdef HAVE_DETAILED_TIMINGS
#ifdef DOUBLE_PRECISION_COMPLEX
      call timer%stop("trans_ev_complex_double")
#else
      call timer%stop("trans_ev_complex_single")
#endif
#endif

#ifdef DOUBLE_PRECISION_COMPLEX
    end subroutine trans_ev_complex_double
#else
    end subroutine trans_ev_complex_single
#endif

#ifdef DOUBLE_PRECISION_COMPLEX
    subroutine hh_transform_complex_double(alpha, xnorm_sq, xf, tau)
#else
    subroutine hh_transform_complex_single(alpha, xnorm_sq, xf, tau)
#endif

      ! Similar to LAPACK routine ZLARFP, but uses ||x||**2 instead of x(:)
      ! and returns the factor xf by which x has to be scaled.
      ! It also hasn't the special handling for numbers < 1.d-300 or > 1.d150
      ! since this would be expensive for the parallel implementation.
      use precision
#ifdef HAVE_DETAILED_TIMINGS
      use timings
#endif
      implicit none
      complex(kind=COMPLEX_DATATYPE), intent(inout) :: alpha
      real(kind=REAL_DATATYPE), intent(in)       :: xnorm_sq
      complex(kind=COMPLEX_DATATYPE), intent(out)   :: xf, tau

      real(kind=REAL_DATATYPE)                   :: ALPHR, ALPHI, BETA
#ifdef HAVE_DETAILED_TIMINGS
#ifdef DOUBLE_PRECISION_COMPLEX
      call timer%start("hh_transform_complex_double")
#else
      call timer%start("hh_transform_complex_single")
#endif
#endif
      ALPHR = real( ALPHA, kind=REAL_DATATYPE )
#ifdef DOUBLE_PRECISION_COMPLEX
      ALPHI = DIMAG( ALPHA )
#else
      ALPHI = AIMAG( ALPHA )
#endif
      if ( XNORM_SQ==0. .AND. ALPHI==0. ) then

        if ( ALPHR>=0. ) then
          TAU = 0.
        else
          TAU = 2.
          ALPHA = -ALPHA
        endif
        XF = 0.

      else

        BETA = SIGN( SQRT( ALPHR**2 + ALPHI**2 + XNORM_SQ ), ALPHR )
        ALPHA = ALPHA + BETA
        IF ( BETA<0 ) THEN
          BETA = -BETA
          TAU = -ALPHA / BETA
        ELSE
#ifdef DOUBLE_PRECISION_COMPLEX
          ALPHR = ALPHI * (ALPHI/real( ALPHA , kind=rk8))
          ALPHR = ALPHR + XNORM_SQ/real( ALPHA, kind=rk8 )

          TAU = DCMPLX( ALPHR/BETA, -ALPHI/BETA )
          ALPHA = DCMPLX( -ALPHR, ALPHI )
#else
          ALPHR = ALPHI * (ALPHI/real( ALPHA , kind=rk4))
          ALPHR = ALPHR + XNORM_SQ/real( ALPHA, kind=rk4 )

          TAU = CMPLX( ALPHR/BETA, -ALPHI/BETA )
          ALPHA = CMPLX( -ALPHR, ALPHI )
#endif
        END IF
#ifdef DOUBLE_PRECISION_COMPLEX
        XF = 1._rk8/ALPHA
#else
        XF = 1._rk4/ALPHA
#endif
        ALPHA = BETA
      endif

#ifdef HAVE_DETAILED_TIMINGS
#ifdef DOUBLE_PRECISION_COMPLEX
      call timer%stop("hh_transform_complex_double")
#else
      call timer%stop("hh_transform_complex_single")
#endif
#endif

#ifdef DOUBLE_PRECISION_COMPLEX
    end subroutine hh_transform_complex_double
#else
    end subroutine hh_transform_complex_single
#endif

#define ALREADY_DEFINED 1
