%% ELSI Manual

\documentclass{report}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{tcolorbox}
\usepackage{scrextend}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{fancyvrb}
\usepackage{longtable}
\addtokomafont{labelinglabel}{\sffamily}
\titleformat{\chapter}[hang]{\bf\huge}{\thechapter}{2pc}{}
\geometry{left=1.6cm,right=1.6cm,top=1.6cm,bottom=2cm}
\parskip=6pt
\parindent=0pt

\newcommand{\tcb}[1]{\textcolor{blue}{#1}}
\newcommand{\tcr}[1]{\textcolor{red}{#1}}
\newcommand{\api}[1]{\textcolor{blue}{\texttt{#1}}}

\begin{document}
% Title
\title{\includegraphics[scale=0.07]{elsi_logo.png}\\ \vspace{0.5cm} \textbf{ELSI Interface User's Guide}}
\author{The ELSI Team\\ \url{http://elsi-interchange.org}}
\maketitle

% Table of contents
\tableofcontents

% Chapter1
\chapter{Introduction}
\section{ELSI: ELectronic Structure Infrastructure}
\label{sec:elsi}
Computer simulations based on electronic structure theory, particularly Kohn-Sham density-functional theory (KS-DFT), are facilitating scientific discoveries across a broad range of disciplines such as chemistry, physics, and materials science. Despite its remarkable success, routine application of KS-DFT to systems consisting of thousands of atoms is still difficult. The major computational bottleneck is an eigenvalue problem
\begin{equation}
\label{eq:ks}
\boldsymbol{H} \boldsymbol{C} = \boldsymbol{S} \boldsymbol{C} \boldsymbol{\Sigma},
\end{equation}

where $\boldsymbol{H}$ and $\boldsymbol{S}$ are the so-called Hamiltonian and overlap matrices, $\boldsymbol{C}$ and $\boldsymbol{\Sigma}$ are the eigenvectors and eigenvalues of this eigensystem. The direct solution of Eq.~\ref{eq:ks} scales cubically with respect to the problem size. To overcome this bottleneck, researchers and software developers are actively improving the efficiency of eigensolvers and developing alternative algorithms that circumvent the explicit solution of the eigenproblem. The open-source ELSI library features a unified software interface that connects electronic structure codes to various high-performance solver libraries ranging from conventional cubic scaling eigensolvers to linear scaling density matrix solvers~\cite{elsi_yu_2018}. To date, it is adopted by four electronic structure packages (DFTB+~\cite{dftb_aradi_2007}, DGDFT~\cite{dgdft_hu_2015}, FHI-aims~\cite{fhiaims_blum_2009}, and SIESTA~\cite{siesta_soler_2002}).

\section{Solver Libraries Supported by ELSI}
\label{sec:solvers}
Distributed-memory solvers supported in the current version of ELSI are: ELPA~\cite{elpa_auckenthaler_2011,elpa_marek_2014,elpa_kus_2019a,elpa_kus_2019b}, libOMM~\cite{libomm_corsetti_2014}, PEXSI~\cite{pexsi_lin_2009,pexsi_lin_2013,pexsi_lin_2014,pselinv_jacquelin_2016,pexsi_jia_2017}, EigenExa~\cite{eigenexa_imamura_2011,eigenexa_fukaya_2015}, SLEPc-SIPc~\cite{slepc_hernandez_2005,sips_campos_2012,sips_keceli_2016,sips_keceli_2018}, NTPoly~\cite{ntpoly_dawson_2018}, and BSEPACK~\cite{bsepack_shao_2016}. Shared-memory solvers supported in the current version of ELSI are: LAPACK~\cite{lapack_anderson_1999} and MAGMA~\cite{magma_tomov_2010,magma_dongarra_2014}.

What follows is a brief summary of the solvers supported in ELSI. For technical descriptions of the solvers, the reader is referred to the original publications of the solvers, e.g., those in the reference list of this document.

\href{https://elpa.mpcdf.mpg.de}{\tcb{ELPA}}: The massively parallel dense eigensolver ELPA facilitates the solution of symmetric or Hermitian eigenproblems on high-performance computers. It features an efficient two-stage tridiagonalization algorithm which is better suited for parallel computing than the conventional one-stage algorithm.

\href{https://esl.cecam.org/LibOMM}{\tcb{libOMM}}: The orbital minimization method (OMM) bypasses the explicit solution of the Kohn-Sham eigenproblem by efficient iterative algorithms which directly minimize an unconstrained energy functional using a set of auxiliary Wannier functions. The Wannier functions are defined on the occupied subspace of the system, reducing the size of the problem. The density matrix is then obtained directly, without calculating the Kohn-Sham orbitals.

\href{https://pexsi.readthedocs.io/en/latest}{\tcb{PEXSI}}: PEXSI is a Fermi operator expansion (FOE) based method which expands the density matrix in terms of a linear combination of a small number of rational functions (pole expansion). Evaluation of these rational functions exploits the sparsity of the Hamiltonian and overlap matrices using selected inversion to enable scaling to 100,000+ of MPI tasks for calculation of the electron density, energy, and forces in electronic structure calculations.

\href{https://www.r-ccs.riken.jp/labs/lpnctrt/en/projects/eigenexa}{\tcb{EigenExa}}: The EigenExa library consists of two massively parallel implementations of direct, dense eigensolver. Its eigen\_sx method features an efficient transformation from full to pentadiagonal matrix. Eigenvalues and eigenvectors of the pentadiagonal matrix are directly solved with a divide-and-conquer algorithm. This method is particularly efficient when a large part of the eigenspectrum is of interest.

\href{http://slepc.upv.es}{\tcb{SLEPc-SIPs}}: SLEPc-SIPs is a parallel sparse eigensolver for real symmetric generalized eigenvalue problems. It implements a distributed spectrum slicing method and it is currently available through the SLEPc library built on top of the PETSc framework.

\href{https://william-dawson.github.io/NTPoly}{\tcb{NTPoly}}: NTPoly is a massively parallel library for computing the functions of sparse, symmetric matrices based on polynomial expansions. For sufficiently sparse matrices, most of the matrix functions can be computed in linear time. Distributed memory parallelization is based on a communication avoiding sparse matrix multiplication algorithm. Various density matrix purification algorithms which compute the density matrix as a function of the Hamiltonian matrix are implemented in NTPoly.

\href{https://sites.google.com/a/lbl.gov/bsepack}{\tcb{BSEPACK}}: BSEPACK is a parallel ScaLAPACK-style library for solving the Bethe-Salpeter eigenvalue problem on distributed-memory high-performance computers.

\href{https://www.netlib.org/lapack}{\tcb{LAPACK}}: LAPACK provides routines for solving linear systems, least squares problems, eigenvalue problems, and singular value problems. In order to promotes high efficiency on present-day computers, LAPACK routines are written to exploit BLAS, particularly level-3 BLAS, as much as possible. In ELSI, the tridiagonalization and the corresponding back-transformation routines in LAPACK are combined with the efficient divide-and-conquer tridiagonal solver in ELPA.

\href{https://icl.utk.edu/magma}{\tcb{MAGMA}}: The MAGMA project aims to develop a dense linear algebra framework for heterogeneous architectures consisting of manycore and GPU systems. MAGMA incorporates the latest advances in synchronization-avoiding and communication-avoiding algorithms, and uses a hybridization methodology where algorithms are split into tasks of varying granularity and their execution scheduled over the available hardware components.

\section{Citing ELSI}
\label{sec:cite}
Key concepts of ELSI and the first version of its implementation are described in the following paper~\cite{elsi_yu_2018}:

V. W-z. Yu, F. Corsetti, A. Garc\'{i}a, W. P. Huhn, M. Jacquelin, W. Jia, B. Lange, L. Lin, J. Lu, W. Mi, A. Seifitokaldani, \'{A}. V\'{a}zquez-Mayagoitia, C. Yang, H. Yang, and V. Blum, ELSI: A Unified Software Interface for Kohn-Sham Electronic Structure Solvers, Computer Physics Communications, 222, 267-285 (2018).

In addition, an incomplete list of publications describing the solvers supported in ELSI may be found in the bibliography of this document. Please consider citing these articles when publishing results obtained with ELSI.

\section{Acknowledgments}
\label{sec:thanks}
ELSI is a National Science Foundation Software Infrastructure for Sustained Innovation - Scientific Software Integration (SI2-SSI) supported software infrastructure project. The ELSI Interface software and this User's Guide are based upon work supported by the National Science Foundation under Grant Number 1450280. Any opinions, findings, and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect the views of the National Science Foundation.

% Chapter2
\chapter{Installation of ELSI}
\section{Prerequisites}
\label{sec:prereq}
The ELSI package contains the ELSI interface software as well as redistributed source code for the solver libraries ELPA (version 2020.05.001), libOMM (version 1.0.0), PEXSI (version 1.2.0), and NTPoly (version 2.4.0). The installation of ELSI makes use of the \href{http://cmake.org}{CMake} software. Minimum requirements include:
\begin{Verbatim}[commandchars=\\\{\}]
\tcb{CMake}  [minimum version 3.0; newer version recommended]
\tcb{Fortran compiler}  [Fortran 2003 compliant]
\tcb{C compiler}  [C99 compliant]
\tcb{MPI}  [MPI-3 recommended]
\end{Verbatim}

The PEXSi, EigenExa, SLEPc-SIPs, BSEPACK, and MAGMA solvers are not enabled by default. Enabling the PEXSI solver requires:
\begin{Verbatim}[commandchars=\\\{\}]
\tcb{C++ compiler}  [C++ 11 compliant]
\end{Verbatim}

Enabling the EigenExa solver requires:
\begin{Verbatim}[commandchars=\\\{\}]
\tcb{EigenExa}  [version 2.3 or newer]
\end{Verbatim}

Enabling the SLEPc-SIPs solver requires:
\begin{Verbatim}[commandchars=\\\{\}]
\tcb{SLEPc}  [version 3.9 or newer]
\tcb{PETSc}  [version 3.9 or newer, with MUMPS and ParMETIS enabled]
\end{Verbatim}

Enabling the MAGMA solver requires:
\begin{Verbatim}[commandchars=\\\{\}]
\tcb{MAGMA}  [version 2.5 or newer]
\tcb{CUDA}  [version 10.0 or newer]
\end{Verbatim}

Linear algebra libraries should be provided for ELSI to link against:
\begin{Verbatim}[commandchars=\\\{\}]
\tcb{BLAS, LAPACK, BLACS, ScaLAPACK}
\end{Verbatim}

\section{Quick Start}
\label{sec:quick}
We recommend preparing configuration settings in a toolchain file that can be read by CMake. Edit one of the templates provided in the ``\texttt{toolchains}'' directory of the ELSI package. As an example, a minimal Intel toolchain looks like
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
# Modify contents in \tcr{red} if necessary
set(CMAKE_Fortran_COMPILER \tcr{"mpiifort"} CACHE STRING "MPI Fortran compiler")
set(CMAKE_Fortran_FLAGS \tcr{"-O3 -ip -fp-model precise"} CACHE STRING "Fortran flags")
set(CMAKE_C_COMPILER \tcr{"mpiicc"} CACHE STRING "MPI C compiler")
set(CMAKE_C_FLAGS \tcr{"-O3 -ip -fp-model precise -std=c99"} CACHE STRING "C flags")
set(LIB_PATHS \tcr{"$ENV{MKLROOT}/lib/intel64"} CACHE STRING "External library paths")
set(LIBS \tcr{"mkl_scalapack_lp64 mkl_blacs_intelmpi_lp64 mkl_intel_lp64 mkl_sequential mkl_core"}
    CACHE STRING "External libraries")
\end{Verbatim}
\end{tcolorbox}

This will build ELSI with the redistributed ELPA, libOMM, and NTPoly solvers. A complete list of configure options may be found in Sec.~\ref{subsec:config_options}.

Once a toolchain file is ready, follow the steps below:
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
$ \tcb{cd elsi-interface}
$ \tcb{ls}

  CMakeLists.txt  external/  src/  test/  ...

$ \tcb{mkdir build}
$ \tcb{cd build}
$ \tcb{cmake -DCMAKE_TOOLCHAIN_FILE=YOUR_TOOLCHAIN_FILE ..}

  ...
  ...
  -- Generating done
  -- Build files have been written to: /current/dir

$ \tcb{make [-j np]}
$ \tcb{[make install]}
\end{Verbatim}
\end{tcolorbox}

``\texttt{YOUR\_TOOLCHAIN\_FILE}'' should be the user's toolchain file. Commands in square brackets are optional.

If the compilation succeeds, the next step would be reading the code examples in the ``\texttt{test}'' directory of the ELSI package, which showcase the use of ELSI in C and Fortran programs.

\section{Configuration}
\label{sec:config}
\subsection{Compilers}
\label{subsec:config_compilers}
CMake automatically detects compilers. The choices made by CMake often work, but they do not necessarily lead to the optimal performance. In some cases, the compilers picked up by CMake may not be the ones desired by the user. To build ELSI, it is mandatory that the user explicitly sets the identification of the compilers in \texttt{CMAKE\_Fortran\_COMPILER}, \texttt{CMAKE\_C\_COMPILER}, and \texttt{CMAKE\_CXX\_COMPILER}. Please note that the C++ compiler is not needed when building ELSI without PEXSI.

In addition, it is highly recommended to specify the compiler flags in \texttt{CMAKE\_Fortran\_FLAGS}, \texttt{CMAKE\_C\_FLAGS}, and \texttt{CMAKE\_CXX\_FLAGS}.

\subsection{Solvers}
\label{subsec:config_solvers}
The ELPA, libOMM, PEXSI, NTPoly, and BSEPACK solver libraries, as well as the SuperLU\_DIST and PT-SCOTCH libraries (both required by PEXSI), are redistributed with the current ELSI package.

The redistributed version of ELPA comes with a few ``kernels'' specifically written to take advantage of processor architecture (e.g. vectorization instruction set extensions), which may be chosen by the \texttt{ELPA2\_KERNEL} keyword. Available options are \texttt{AVX}, \texttt{AVX2}, and \texttt{AVX512}, for architectures supporting Intel AVX, AVX2, and AVX512 instruction sets, respectively. In ELPA, these kernels are employed to accelerate the calculation of eigenvectors, which is often a bottleneck when calculating a large portion of the eigenspectrum.

The PEXSI, EigenExa, SLEPc-SIPs, BSEPACK, and MAGMA solvers are not enabled by default. They may be activated by the keywords \texttt{ENABLE\_PEXSI}, \texttt{ENABLE\_EIGENEXA}, \texttt{ENABLE\_SIPS}, \texttt{ENABLE\_BSEPACK}, and \texttt{ENABLE\_MAGMA}, respectively. PEXSI 1.2.0, EigenExa 2.3, 2.4, SLEPc 3.9, 3.10, 3.11, 3.12, 3.13, BSEPACK 0.1, and MAGMA 2.5 have been tested with this version of ELSI. Older/newer versions may or may not be compatible. The PETSc library, required by SLEPc, must be compiled with MPI support, and (at least) with MUMPS and ParMETIS enabled.

Experienced users are encouraged to link ELSI against externally installed, better optimized solver libraries. The keywords \texttt{USE\_EXTERNAL\_ELPA}, \texttt{USE\_EXTERNAL\_OMM}, \texttt{USE\_EXTERNAL\_PEXSI}, \texttt{USE\_EXTERNAL\_NTPOLY}, and \texttt{USE\_EXTERNAL\_BSEPACK} control the usage of externally compiled ELPA, libOMM, PEXSI, NTPoly, and BSEPACK, respectively.

All external libraries and include paths should be set via \texttt{INC\_PATHS}, \texttt{LIB\_PATHS}, and \texttt{LIBS}, each of which is a list of items separated with `` '' (space) or ``;'' (semicolon). If an external library depends on additional libraries, \texttt{LIBS} should include all the relevant dependencies. For instance, \texttt{LIBS} should include the MAGMA library and CUDA libraries when enabling MAGMA support.

\subsection{Tests}
\label{subsec:config_tests}
Building ELSI test programs may be enabled by \texttt{ENABLE\_TESTS}. Then, the compilation of ELSI may be verified by ``\texttt{make test}'' or ``\texttt{ctest}''. Note that the tests may not run if launching MPI jobs is prohibited on the user's working platform.

\subsection{List of All Configure Options}
\label{subsec:config_options}
The options accepted by the ELSI CMake build system are listed here in alphabetical order. Some additional explanations are made below the table.

\begin{longtable}[]{|p{40mm}|p{15mm}|p{20mm}|p{87mm}|}
\hline
\multicolumn{1}{|l|}{\textbf{Option}} & \multicolumn{1}{l|}{\textbf{Type}} & \multicolumn{1}{l|}{\textbf{Default}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{ADD\_UNDERSCORE}          & boolean & ON          & Suffix C functions with an underscore\\
\hline
\texttt{BUILD\_SHARED\_LIBS}      & boolean & OFF         & Build ELSI as a shared library\\
\hline
\texttt{CMAKE\_C\_COMPILER}       & string  & none        & MPI C compiler\\
\hline
\texttt{CMAKE\_C\_FLAGS}          & string  & none        & C flags\\
\hline
\texttt{CMAKE\_CXX\_COMPILER}     & string  & none        & MPI C++ compiler\\
\hline
\texttt{CMAKE\_CXX\_FLAGS}        & string  & none        & C++ flags\\
\hline
\texttt{CMAKE\_Fortran\_COMPILER} & string  & none        & MPI Fortran compiler\\
\hline
\texttt{CMAKE\_Fortran\_FLAGS}    & string  & none        & Fortran flags\\
\hline
\texttt{CMAKE\_INSTALL\_PREFIX}   & path    & /usr/local  & Path to install ELSI\\
\hline
\texttt{ELPA2\_KERNEL}            & string  & none        & ELPA2 kernel\\
\hline
\texttt{ENABLE\_BSEPACK}          & boolean & OFF         & Enable BSEPACK support\\
\hline
\texttt{ENABLE\_C\_TESTS}         & boolean & OFF         & Build C test programs\\
\hline
\texttt{ENABLE\_EIGENEXA}         & boolean & OFF         & Enable EigenExa support\\
\hline
\texttt{ENABLE\_MAGMA}            & boolean & OFF         & Enable MAGMA support\\
\hline
\texttt{ENABLE\_PEXSI}            & boolean & OFF         & Enable PEXSI support\\
\hline
\texttt{ENABLE\_SIPS}             & boolean & OFF         & Enable SLEPc-SIPs support\\
\hline
\texttt{ENABLE\_TESTS}            & boolean & OFF         & Build Fortran test programs\\
\hline
\texttt{INC\_PATHS}               & string  & none        & Include directories of external libraries\\
\hline
\texttt{LIB\_PATHS}               & string  & none        & Directories containing external libraries\\
\hline
\texttt{LIBS}                     & string  & none        & External libraries\\
\hline
\texttt{MPIEXEC\_NP}              & string  & mpirun -n 4 & Command to run tests in parallel with MPI\\
\hline
\texttt{MPIEXEC\_1P}              & string  & mpirun -n 1 & Command to run tests in serial with MPI\\
\hline

\texttt{SCOTCH\_LAST\_RESORT}     & string  & none        & Command to invoke PT-SCOTCH header generator\\
\hline
\texttt{USE\_EXTERNAL\_BSEPACK}   & boolean & OFF         & Use external BSEPACK\\
\hline
\texttt{USE\_EXTERNAL\_ELPA}      & boolean & OFF         & Use external ELPA\\
\hline
\texttt{USE\_EXTERNAL\_NTPOLY}    & boolean & OFF         & Use external NTPoly\\
\hline
\texttt{USE\_EXTERNAL\_OMM}       & boolean & OFF         & Use external libOMM and MatrixSwitch\\
\hline
\texttt{USE\_EXTERNAL\_PEXSI}     & boolean & OFF         & Use external PEXSI (if PEXSI enabled)\\
\hline
\texttt{USE\_MPI\_MODULE}         & boolean & OFF         & Use MPI module instead of ``\texttt{mpif.h}'' in Fortran code\\
\hline
\end{longtable}

\textbf{Remarks}

(1) \texttt{ADD\_UNDERSCORE}: In the redistributed PEXSI and SuperLU\_DIST code, there are calls to functions from the linear algebra libraries, e.g. ``dgemm''. If \texttt{ADD\_UNDERSCORE} is ``ON'', the code will call ``dgemm\_'' instead of ``dgemm''. Turn this keyword off if routines are not suffixed with ``\_'' in the linear algebra libraries.

(2) \texttt{CMAKE\_INSTALL\_PREFIX}: ELSI may be installed to the location specified in \texttt{CMAKE\_INSTALL\_PREFIX} by ``\texttt{make install}''.

(3) \texttt{ELPA2\_KERNEL}: There are a number of computational kernels available with the ELPA solver. Choose from ``\texttt{AVX}'' (Intel AVX), ``\texttt{AVX2}'' (Intel AVX2), and ``\texttt{AVX512}'' (Intel AVX512). See Sec.~\ref{subsec:config_solvers} for more information.

(4) \texttt{SCOTCH\_LAST\_RESORT}: The compilation of PT-SCOTCH is a multi-step process. First, two auxiliary executables are created. Then, some header files are generated on-the-fly by the two executables. Finally, the main source files are compiled with the generated header files included. The header generation step may fail on platforms where directly running an executable is prohibited, e.g. login nodes of a supercomputer. Often this can be circumvented by requesting an interactive session to a compute node and compiling the code there, or by submitting the compilation as a job to the queuing system. However, this may still fail on platforms where an executable compiled with MPI must be launched by an MPI job launcher (aprun, mpirun, srun, etc). If the standard compilation of PT-SCOTCH fails due to this reason, the user may set \texttt{SCOTCH\_LAST\_RESORT} to the command that starts an MPI job with one MPI task, e.g. ``\texttt{mpirun -n 1}''. This command is then used to launch the auxiliary executables to generate necessary header files for PT-SCOTCH.

(5) External libraries: ELSI redistributes source code of ELPA, libOMM, NTPoly, PEXSI, SuperLU\_DIST, and PT-SCOTCH libraries, which are built by default together with the ELSI interface. Experienced users are encouraged to link the ELSI interface against external, better optimized solver libraries. See Sec.~\ref{subsec:config_solvers} for more information.

\section{Importing ELSI into Third-Party Code Projects}
\label{sec:import}
\subsection{Linking against ELSI: CMake}
\label{subsec:import_cmake}
A CMake configuration file called \texttt{elsiConfig.cmake} should be generated after ELSI is successfully installed. This file contains all the information about how the ELSI library and its dependencies should be included in an external project. For a project using CMake, only two lines are required to find and link to ELSI:
\begin{tcolorbox}
\begin{verbatim}
find_package(elsi REQUIRED)
target_link_libraries(my_project PRIVATE elsi::elsi)
\end{verbatim}
\end{tcolorbox}

If a minimum version of ELSI is required, this information may be passed to ``\texttt{find\_package}'' by, e.g.:
\begin{tcolorbox}
\begin{verbatim}
find_package(elsi 2.0 REQUIRED)
\end{verbatim}
\end{tcolorbox}

\subsection{Linking against ELSI: Makefile}
\label{subsec:import_makefile}
For a project using makefiles, an example set of compiler flags to link against ELSI would be:
\begin{tcolorbox}
\begin{verbatim}
ELSI_INCLUDE = -I/PATH/TO/BUILD/ELSI/include
ELSI_LIB     = -L/PATH/TO/BUILD/ELSI/lib -lelsi \
               -lfortjson -lOMM -lMatrixSwitch -lelpa \
               -lNTPoly -lpexsi -lsuperlu_dist \
               -lptscotchparmetis -lptscotch -lptscotcherr \
               -lscotchmetis -lscotch -lscotcherr
\end{verbatim}
\end{tcolorbox}

Enabling/disabling PEXSI, EigenExa, SLEPc-SIPs, BSEPACK, MAGMA or linking ELSI against externally installed solver libraries requires the user modify these flags accordingly.

\subsection{Using ELSI}
\label{subsec:import_use}
ELSI may be used in an electronic structure code by importing the appropriate header file. For codes written in Fortran, this is done by using the ELSI module
\begin{tcolorbox}
\begin{verbatim}
use ELSI
\end{verbatim}
\end{tcolorbox}

For codes written in C, the ELSI wrapper may be imported by including the header file
\begin{tcolorbox}
\begin{verbatim}
#include <elsi.h>
\end{verbatim}
\end{tcolorbox}

% Chapter3
\chapter{The ELSI API}
\section{Overview of the ELSI API}
\label{sec:api}
In this chapter, we present the public-facing API for the ELSI Interface. We anticipate that fine details of this interface may change slightly in the future, but the fundamental structure of the interface layer is expected to remain consistent. While this chapter serves as a reference to the ELSI subroutines, the user is encouraged to explore the demonstration pseudo-codes of ELSI in Sec.~\ref{sec:example}.

To allow multiple instances of ELSI to co-exist within a single calling code, we define an \texttt{elsi\_handle} data type to encapsulate the state of an ELSI instance, i.e., all runtime parameters associated with the ELSI instance. An \texttt{elsi\_handle} instance is initialized with the \texttt{elsi\_init} subroutine and is subsequently passed to all other ELSI subroutine calls.

ELSI provides a C interface in addition to the native Fortran interface. The vast majority of this chapter, while written from a Fortran standpoint, applies equally to both interfaces. Information specifically about the C wrapper for ELSI may be found in Sec.~\ref{sec:c}.

In the source code of ELSI, there may exist subroutines that are not documented as public API here. Usage of those undocumented subroutines is not recommended, as they are usually experimental and subject to modification or removal without notice.

\section{Setting Up ELSI}
\label{sec:setup}
\subsection{Initializing ELSI}
\label{subsec:setup_init}
The ELSI interface must be initialized via the \texttt{elsi\_init} subroutine before any other ELSI subroutine may be called.

\begin{tabular}[]{|p{25mm}|p{20mm}|p{10mm}|p{107mm}|}
\multicolumn{4}{l}{\api{elsi\_init}(handle, solver, parallel\_mode, matrix\_format, n\_basis, n\_electron, n\_state)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}         & elsi\_handle & out & ELSI handle.\\
\hline
\texttt{solver}         & integer      & in  & 0: \texttt{AUTO}. 1: \texttt{ELPA}. 2: \texttt{libOMM}. 3: \texttt{PEXSI}. 4: \texttt{EigenExa}. 5: \texttt{SLEPc-SIPs}. 6: \texttt{NTPoly}. 7: \texttt{MAGMA}. 8: \texttt{BSEPACK}. See remark 1.\\
\hline
\texttt{parallel\_mode} & integer      & in  & 0: \texttt{SINGLE\_PROC}. 1: \texttt{MULTI\_PROC}. See remark 4.\\
\hline
\texttt{matrix\_format} & integer      & in  & 0: \texttt{BLACS\_DENSE}. 1: \texttt{PEXSI\_CSC}. 2: \texttt{SIESTA\_CSC}. 3: \texttt{GENERIC\_COO}. See remark 2.\\
\hline
\texttt{n\_basis}       & integer      & in  & Number of basis functions, i.e. global size of Hamiltonian.\\
\hline
\texttt{n\_electron}    & real double  & in  & Number of electrons.\\
\hline
\texttt{n\_state}       & integer      & in  & Number of states. See remark 3.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) \texttt{solver}: The ``\texttt{AUTO}'' option attempts to automate the solver selection procedure based on benchmarks performed and experiences gained in the ELSI project. User-supplied information may assist in finding the optimal solver. In particular, see \texttt{elsi\_set\_dimensionality} and \texttt{elsi\_set\_energy\_gap} in Sec.~\ref{sec:setter}. Simply put, the solver selection favors ELPA for small-and-medium-sized problems, PEXSI for large, sparse, low-dimensional problems, and NTPoly for extra-large, sparse systems with a decent energy gap.

When the ELPA solver is chosen for the ``\texttt{SINGLE\_PROC}'' parallel mode, the tridiagonalization and back-transformation routines in LAPACK and the divide-and-conquer tridiagonal solver routine in ELPA are used.

The table below summarizes the supported problem types for each solver.

\begin{tabular}[]{|p{32mm}|p{32mm}|p{32mm}|p{32mm}|p{32mm}|}
\hline
\multicolumn{1}{|l|}{\textbf{Solver}} & \multicolumn{1}{l|}{\textbf{Parallel Mode}} & \multicolumn{1}{l|}{\textbf{Matrix Format}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Problem Type}}\\
\hline
\texttt{ELPA}       & \texttt{SINGLE\_PROC} & \texttt{BLACS\_DENSE} & Real/complex & KS-EV\\
\hline
\texttt{ELPA}       & \texttt{MULTI\_PROC}  & All                   & Real/complex & KS-EV/KS-DM\\
\hline
\texttt{libOMM}     & \texttt{MULTI\_PROC}  & All                   & Real/complex & KS-DM\\
\hline
\texttt{PEXSI}      & \texttt{MULTI\_PROC}  & All                   & Real/complex & KS-DM\\
\hline
\texttt{EigenExa}   & \texttt{MULTI\_PROC}  & All                   & Real         & KS-EV/KS-DM\\
\hline
\texttt{SLEPc-SIPs} & \texttt{MULTI\_PROC}  & All                   & Real         & KS-EV/KS-DM\\
\hline
\texttt{NTPoly}     & \texttt{MULTI\_PROC}  & All                   & Real/complex & KS-DM\\
\hline
\texttt{MAGMA}      & \texttt{SINGLE\_PROC} & \texttt{BLACS\_DENSE} & Real/complex & KS-EV\\
\hline
\texttt{BSEPACK}    & \texttt{MULTI\_PROC}  & \texttt{BLACS\_DENSE} & Real/complex & BSE-EV\\
\hline
\end{tabular}

(2) \texttt{matrix\_format}: ``\texttt{BLACS\_DENSE}'' refers to a dense matrix format in a 2-dimensional block-cyclic distribution, i.e. the BLACS standard. ``\texttt{PEXSI\_CSC}'' refers to a compressed sparse column (CSC) matrix format in a 1-dimensional block distribution. ``\texttt{SIESTA\_CSC}'' refers to a compressed sparse column (CSC) matrix format in a 1-dimensional block-cyclic distribution. As the Hamiltonian, overlap, and density matrices are symmetric (Hermitian), compressed sparse row (CSR) matrix format is effectively supported. ``\texttt{GENERIC\_COO}'' refers to a coordinate (COO) sparse matrix format in an arbitrary distribution. Please refer to Sec.~\ref{subsec:setup_matrix} for specifications of these matrix formats.

(3) \texttt{n\_state}: If ELPA, EigenExa, SLEPc-SIPs, or MAGMA is the chosen solver, this parameter specifies the number of eigenstates to solve. EigenExa internally computes all the eigenstates unless \texttt{n\_state} is 0. When \texttt{n\_state} is larger than 0 and smaller than \texttt{n\_basis}, ELSI simply discards the unwanted solutions. libOMM, PEXSI and NTPoly do not make use of this parameter.

(4) \texttt{parallel\_mode}: The two parallelization modes, ``\texttt{SINGLE\_PROC}'' and ``\texttt{MULTI\_PROC}'', allow for two parallelization strategies commonly employed by electronic structure codes. See below.

\textbf{4a)} ``\texttt{SINGLE\_PROC}'': Solves the KS eigenproblem following a LAPACK-like fashion. This option may only be selected when ELPA or MAGMA is chosen as the solver. Every MPI task independently handles a group of $k$-points uniquely assigned to it. Example: 16 $k$-points, 4 MPI tasks.
\begin{itemize}
\item MPI task 0 handles $k$-points 1, 2, 3, 4 sequentially;
\item MPI task 1 handles $k$-points 5, 6, 7, 8 sequentially;
\item MPI task 2 handles $k$-points 9, 10, 11, 12 sequentially;
\item MPI task 3 handles $k$-points 13, 14, 15, 16 sequentially.
\end{itemize}

\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
call \tcb{elsi_init} (eh, ..., parallel_mode=0, ...)
...
do i_kpt = 1, n_kpt_local
   call \tcb{elsi_ev_\{real|complex\}} (eh, ham_this_kpt, ovlp_this_kpt, eval_this_kpt, evec_this_kpt)
end do
\end{Verbatim}
\end{tcolorbox}

\textbf{4b)} ``\texttt{MULTI\_PROC}'': Solves the KS eigenproblem following a ScaLAPACK-like fashion. Groups of MPI tasks coordinate to handle the same $k$-point, uniquely assigned to that group. Example: 4 $k$-points, 16 MPI tasks.
\begin{itemize}
\item MPI tasks 0, 1, 2, 3 cooperatively handle $k$-point 1;
\item MPI tasks 4, 5, 6, 7 cooperatively handle $k$-point 2;
\item MPI tasks 8, 9, 10, 11 cooperatively handle $k$-point 3;
\item MPI tasks 12, 13, 14, 15 cooperatively handle $k$-point 4.
\end{itemize}

\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
call \tcb{elsi_init} (eh, ..., parallel_mode=1, ...)
call \tcb{elsi_set_mpi} (eh, my_mpi_comm)
call \tcb{elsi_set_kpoint} (eh, n_kpt, my_kpt, my_weight)
call \tcb{elsi_set_mpi_global} (eh, mpi_comm_global)
...
call \tcb{elsi_\{ev|dm\}_\{real|complex\}} (eh, my_ham, my_ovlp, ...)
\end{Verbatim}
\end{tcolorbox}

Please note that when there is more than one $k$-point, a global MPI communicator must be provided for inter-$k$-point communications. See Sec.~\ref{subsec:setup_kpt} for \api{elsi\_set\_kpoint}, \api{elsi\_set\_spin}, and \api{elsi\_set\_mpi\_global}, which are used to set up a calculation with two spin channels and/or multiple $k$-points.

\subsection{Setting Up MPI}
\label{subsec:setup_mpi}
The MPI communicator used by ELSI is passed into ELSI by the calling code via the \api{elsi\_set\_mpi} subroutine. When there is more than one $k$-point and/or spin channel, this communicator is used only for solving one problem corresponding to one $k$-point and one spin channel. See Sec.~\ref{subsec:setup_kpt} for details.

\begin{tabular}[]{|p{20mm}|p{20mm}|p{10mm}|p{112mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_mpi}(handle, comm)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle & inout & ELSI handle.\\
\hline
\texttt{comm}   & integer      & in    & MPI communicator.\\
\hline
\end{tabular}

\subsection{Setting Up Matrix Formats}
\label{subsec:setup_matrix}
Four matrix formats are supported by ELSI, namely the 2D block-cyclic distributed dense matrix format (``\texttt{BLACS\_DENSE}''), the 1D block distributed compressed sparse column format (``\texttt{PEXSI\_CSC}''), the 1D block-cyclic distributed compressed sparse column format (``\texttt{SIESTA\_CSC}''), and the arbitrarily distributed coordinate sparse format (``\texttt{GENERIC\_COO}'').

When using the ``\texttt{BLACS\_DENSE}'' format, BLACS parameters are passed into ELSI via the \api{elsi\_set\_blacs} subroutine. The matrix format used internally in the ELSI interface and the ELPA solver requires the block sizes of the 2-dimensional block-cyclic distribution are the same in the row and column directions. It is necessary to call this subroutine before calling any solver interface that makes use of the ``\texttt{BLACS\_DENSE}'' format.

\begin{tabular}[]{|p{20mm}|p{20mm}|p{10mm}|p{112mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_blacs}(handle, blacs\_ctxt, block\_size)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}      & elsi\_handle & inout & ELSI handle.\\
\hline
\texttt{blacs\_ctxt} & integer      & in    & BLACS context.\\
\hline
\texttt{block\_size} & integer      & in    & Block size of the 2D block-cyclic distribution, specifying both row and column directions.\\
\hline
\end{tabular}

When using the ``\texttt{PEXSI\_CSC}'' or ``\texttt{SIESTA\_CSC}'' format, the sparsity pattern should be passed into ELSI via the \api{elsi\_set\_csc} subroutine. It is necessary to call this subroutine before calling any solver interface that makes use of the CSC sparse matrix formats.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_csc}(handle, global\_nnz, local\_nnz, local\_col, row\_idx, col\_ptr)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}      & elsi\_handle     & inout & ELSI handle.\\
\hline
\texttt{global\_nnz} & integer          & in    & Global number of non-zeros.\\
\hline
\texttt{local\_nnz}  & integer          & in    & Local number of non-zeros.\\
\hline
\texttt{local\_col}  & integer          & in    & Local number of matrix columns.\\
\hline
\texttt{row\_idx}    & 1D integer array & in    & Local row index array. Dimension: local\_nnz.\\
\hline
\texttt{col\_ptr}    & 1D integer array & in    & Local column pointer array. Dimension: local\_col+1.\\
\hline
\end{tabular}

The block size of the ``\texttt{PEXSI\_CSC}'' format cannot be set by the user. This is because the PEXSI solver requires that the block size must be floor($\text{N}_\text{basis}$/$\text{N}_\text{procs}$), where floor(x) is the greatest integer less than or equal to x, $\text{N}_\text{basis}$ and $\text{N}_\text{procs}$ are the number of basis functions and the number of MPI tasks, respectively. The block size of the ``\texttt{SIESTA\_CSC}'' format must be explicitly set by calling \api{elsi\_set\_csc\_blk}.

\begin{tabular}[]{|p{20mm}|p{20mm}|p{10mm}|p{112mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_csc\_blk}(handle, block\_size)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}      & elsi\_handle & inout & ELSI handle.\\
\hline
\texttt{global\_nnz} & integer      & in    & Block size of the 1D block-cyclic distribution.\\
\hline
\end{tabular}

In most cases, input and output matrices should be distributed across all MPI tasks. The only exception is when using the PEXSI solver, the sparse density matrix interface \api{elsi\_dm\_\{real$\vert$complex\}\_sparse}, and the ``\texttt{PEXSI\_CSC}'' matrix format. In this case, an additional parameter, \texttt{pexsi\_np\_per\_pole}, must be set by the user. Input and output matrices should be 1D-block-distributed among the first \texttt{pexsi\_np\_per\_pole} MPI tasks (not all the MPI tasks). Please see Sec.~\ref{subsec:setter_pexsi} for more information.

When using the ``\texttt{GENERIC\_COO}'' format, the sparsity pattern should be passed into ELSI via the \api{elsi\_set\_coo} subroutine. It is necessary to call this subroutine before calling any solver interface that makes use of the COO sparse matrix format. The distribution of matrix elements in the ``\texttt{GENERIC\_COO}'' format is arbitrary. Both sorted and unsorted inputs are supported.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_coo}(handle, global\_nnz, local\_nnz, row\_idx, col\_idx)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}      & elsi\_handle     & inout & ELSI handle.\\
\hline
\texttt{global\_nnz} & integer          & in    & Global number of non-zeros.\\
\hline
\texttt{local\_nnz}  & integer          & in    & Local number of non-zeros.\\
\hline
\texttt{row\_idx}    & 1D integer array & in    & Local row index array. Dimension: local\_nnz.\\
\hline
\texttt{col\_idx}    & 1D integer array & in    & Local column index array. Dimension: local\_nnz.\\
\hline
\end{tabular}

\subsection{Setting Up Multiple \textit{k}-points and/or Spin Channels}
\label{subsec:setup_kpt}
When there is more than one $k$-point and/or spin channel in the physical system being simulated, the ELSI interface can be set up to support parallel calculation of the $k$-points and/or spin channels. The base case is a system isolated in space, e.g. free atoms, molecules, clusters, without spin-polarization. In this case, there is one eigenproblem in each iteration of an SCF cycle. When a spin-polarized periodic system is considered, there is an index $\alpha$ denoting the spin channel, and an index $k$ denoting points in reciprocal space. In total, there are $\text{N}_\text{kpt} \times \text{N}_\text{spin}$ eigenproblems that can be solved in an embarrassingly parallel fashion. In ELSI, these eigenproblems are considered as equivalent ``unit tasks''. The available computer processes are divided into $\text{N}_\text{kpt} \times \text{N}_\text{spin}$ groups, each of which is responsible for one unit task.

To set up the ELSI interface for a calculation with more than one $k$-point and/or more than one spin channel, the \api{elsi\_set\_kpoint} and/or \api{elsi\_set\_spin} subroutines are called to pass the required information into ELSI. The MPI communicator for each unit task is passed into ELSI by calling \api{elsi\_set\_mpi}. In addition, a global MPI communicator for all tasks is passed into ELSI by calling \api{elsi\_set\_mpi\_global}. Note that the current ELSI interface only supports the case where the eigenproblems for all the $k$-points and spin channels are fully parallelized, i.e., there is no MPI task handling more than one $k$-point and/or more than one spin channel. In ELSI, the two spin channels are always coupled by a uniform chemical potential. The distribution of electrons among the two channels, and thus the net spin moment of the system, cannot be specified. Calculations with a fixed, user-specified spin moment can be performed by initializing two independent ELSI instances for the two spin channels.

In this version of ELSI, the SLEPc-SIPs eigensolver is not supported in spin-polarized and/or periodic calculations.

\begin{tabular}[]{|p{20mm}|p{20mm}|p{10mm}|p{112mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_kpoint}(handle, n\_kpt, i\_kpt, weight)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle & inout & ELSI handle.\\
\hline
\texttt{n\_kpt} & integer      & in    & Total number of $k$-points.\\
\hline
\texttt{i\_kpt} & integer      & in    & Index of the $k$-point handled by this MPI task.\\
\hline
\texttt{weight} & integer      & in    & Weight of the $k$-point handled by this MPI task.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{20mm}|p{10mm}|p{112mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_spin}(handle, n\_spin, i\_spin)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}  & elsi\_handle & inout & ELSI handle.\\
\hline
\texttt{n\_spin} & integer      & in    & Total number of spin channels.\\
\hline
\texttt{i\_spin} & integer      & in    & Index of the spin channel handled by this MPI task.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{20mm}|p{10mm}|p{112mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_mpi\_global}(handle, comm\_global)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}       & elsi\_handle & inout & ELSI handle.\\
\hline
\texttt{comm\_global} & integer      & in    & Global MPI communicator used for communications among all $k$-points and spin channels.\\
\hline
\end{tabular}

\subsection{Re-initializing ELSI}
\label{subsec:setup_reinit}
When a geometry update takes place in geometry optimization or molecular dynamics calculations, the overlap matrix changes due to the movement of localized basis functions. Calling \textbf{elsi\_reinit} instructs ELSI to flush geometry-related variables and arrays that cannot be used in the new geometry step, e.g., the overlap matrix and its sparsity pattern. Other runtime parameters are kept within the ELSI instance and reused throughout multiple geometry steps. Note that the chemical potential determination in PEXSI must be restarted for every new geometry. See Sec.~\ref{subsec:setter_pexsi} for details.

\begin{tabular}[]{|p{20mm}|p{20mm}|p{10mm}|p{112mm}|}
\multicolumn{4}{l}{\api{elsi\_reinit}(handle)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle & inout & ELSI handle.\\
\hline
\end{tabular}

\subsection{Finalizing ELSI}
\label{subsec:setup_final}
When an ELSI instance is no longer needed, its associated handle should be cleaned up by calling \api{elsi\_finalize}.

\begin{tabular}[]{|p{20mm}|p{20mm}|p{10mm}|p{112mm}|}
\multicolumn{4}{l}{\api{elsi\_finalize}(handle)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle & inout & ELSI handle.\\
\hline
\end{tabular}

\section{Solving Eigenvalues and Eigenvectors}
\label{sec:ev}
\api{elsi\_ev\_\{real$\vert$complex\}\{\_sparse\}} returns all the eigenvalues and a subset of eigenvectors of a generalized eigenproblem defined in Eq.~\ref{eq:ks}. See \api{elsi\_set\_unit\_ovlp} in Sec.~\ref{subsec:setter_elsi} for standard eigenproblems. ELPA, EigenExa, SLEPc-SIPs, or MAGMA may be selected as the solver when using these subroutines.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_ev\_real}(handle, ham, ovlp, eval, evec)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle         & inout & ELSI handle.\\
\hline
\texttt{ham}    & 2D real double array & inout & Hamiltonian matrix in ``\texttt{BLACS\_DENSE}'' format. See remark 1.\\
\hline
\texttt{ovlp}   & 2D real double array & inout & Overlap matrix (or its Cholesky factorization) in ``\texttt{BLACS\_DENSE}'' format. See remark 1.\\
\hline
\texttt{eval}   & 1D real double array & inout & Eigenvalues. See remark 2.\\
\hline
\texttt{evec}   & 2D real double array & out   & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format. See remark 3.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_ev\_complex}(handle, ham, ovlp, eval, evec)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle            & inout & ELSI handle.\\
\hline
\texttt{ham}    & 2D complex double array & inout & Hamiltonian matrix in ``\texttt{BLACS\_DENSE}'' format. See remark 1.\\
\hline
\texttt{ovlp}   & 2D complex double array & inout & Overlap matrix (or its Cholesky factorization) in ``\texttt{BLACS\_DENSE}'' format. See remark 1.\\
\hline
\texttt{eval}   & 1D real double array    & inout & Eigenvalues. See remark 2.\\
\hline
\texttt{evec}   & 2D complex double array & out   & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format. See remark 3.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_ev\_real\_sparse}(handle, ham, ovlp, eval, evec)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle         & inout & ELSI handle.\\
\hline
\texttt{ham}    & 1D real double array & inout & Hamiltonian matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{ovlp}   & 1D real double array & inout & Overlap matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{eval}   & 1D real double array & inout & Eigenvalues. See remark 2.\\
\hline
\texttt{evec}   & 2D real double array & out   & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format. See remark 3.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_ev\_complex\_sparse}(handle, ham, ovlp, eval, evec)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle            & inout & ELSI handle.\\
\hline
\texttt{ham}    & 1D complex double array & inout & Hamiltonian matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{ovlp}   & 1D complex double array & inout & Overlap matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{eval}   & 1D real double array    & inout & Eigenvalues. See remark 2.\\
\hline
\texttt{evec}   & 2D complex double array & out   & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format. See remark 3.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) When using \api{elsi\_ev\_\{real$\vert$complex\}} with ELPA or EigenExa, the Hamiltonian matrix is destroyed during the computation, the overlap matrix is used to store its Cholesky factorization, which can be reused until the overlap matrix changes. When using \api{elsi\_ev\_\{real$\vert$complex\}\_sparse}, the Cholesky factorization is stored internally in the ``\texttt{BLACS\_DENSE}'' format.

(2) The dimension of \texttt{eval} should always be \texttt{n\_basis}, regardless of the choice of \texttt{n\_state} specified in \api{elsi\_init}.

(3) The number of eigenvectors to be computed by \api{elsi\_ev\_\{real$\vert$complex\}\{\_sparse\}} is specified by \texttt{n\_state} in \api{elsi\_init}. However, the local \texttt{evec} array should always be initialized to correspond to a global array of size \texttt{n\_basis} by \texttt{n\_basis}, whose extra part is used as work space. When using \api{elsi\_ev\_\{real$\vert$complex\}\_sparse}, the eigenvectors are returned in a dense format (``\texttt{BLACS\_DENSE}''), as they are in general not sparse.

\api{elsi\_bse\_\{real$\vert$complex\}} solves the Bethe-Salpeter eigenproblem
\begin{equation}
\boldsymbol{H}_\text{BS} \boldsymbol{C} = \boldsymbol{C} \boldsymbol{\Sigma}.
\end{equation}

The BSE Hamiltonian $\boldsymbol{H}_\text{BS}$ has the following structure
\begin{equation}
\boldsymbol{H}_\text{BS} =
\begin{bmatrix}
\boldsymbol{A} & \boldsymbol{B}\\
-\boldsymbol{B}^\text{H} & \boldsymbol{A}^\text{T}
\end{bmatrix},
\end{equation}

where $\boldsymbol{A}$ and $\boldsymbol{B}$ are N by N matrices, $\boldsymbol{H}_\text{BS}$ and $\boldsymbol{C}$ are therefore 2N by 2N matrices. $^\text{H}$ and $^\text{T}$ denote the conjugate transpose and the transpose of a matrix, respectively. $\boldsymbol{A} = \boldsymbol{A}^\text{H}$ and $\boldsymbol{B} = \boldsymbol{B}^\text{T}$. BSEPACK must be selected as the solver when using these subroutines.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_bse\_real}(handle, A, B, eval, evec)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle         & inout & ELSI handle.\\
\hline
\texttt{A}      & 2D real double array & inout & Matrix A in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{B}      & 2D real double array & in    & Matrix B in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{eval}   & 1D real double array & out   & Eigenvalues.\\
\hline
\texttt{evec}   & 2D real double array & out   & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format. See remark 1.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_bse\_complex}(handle, A, B, eval, evec)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle            & inout & ELSI handle.\\
\hline
\texttt{A}      & 2D complex double array & inout & Matrix A in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{B}      & 2D complex double array & in    & Matrix B in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{eval}   & 1D real double array    & out   & Eigenvalues.\\
\hline
\texttt{evec}   & 2D complex double array & out   & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format. See remark 1.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) The global dimension of \texttt{evec} should be 2N by 2N.

\section{Computing Density Matrices}
\label{sec:dm}
\api{elsi\_dm\_\{real$\vert$complex\}\{\_sparse\}} returns the density matrix computed from the provided H and S matrices, as well as the band structure energy.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_dm\_real}(handle, ham, ovlp, dm, e\_bs)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle         & inout & ELSI handle.\\
\hline
\texttt{ham}    & 2D real double array & inout & Hamiltonian matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{ovlp}   & 2D real double array & inout & Overlap matrix (or Cholesky factorization) in ``\texttt{BLACS\_DENSE}'' format. See remark 1.\\
\hline
\texttt{dm}     & 2D real double array & out   & Density matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{e\_bs}  & real double          & out   & Band structure energy.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_dm\_complex}(handle, ham, ovlp, dm, e\_bs)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle            & inout & ELSI handle.\\
\hline
\texttt{ham}    & 2D complex double array & inout & Hamiltonian matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{ovlp}   & 2D complex double array & inout & Overlap matrix (or its Cholesky factorization) in ``\texttt{BLACS\_DENSE}'' format. See remark 1.\\
\hline
\texttt{dm}     & 2D complex double array & out   & Density matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{e\_bs}  & real double             & out   & Band structure energy.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_dm\_real\_sparse}(handle, ham, ovlp, dm, e\_bs)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle         & inout & ELSI handle.\\
\hline
\texttt{ham}    & 1D real double array & inout & Hamiltonian matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{ovlp}   & 1D real double array & inout & Overlap matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{dm}     & 1D real double array & out   & Density matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{e\_bs}  & real double          & out   & Band structure energy.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_dm\_complex\_sparse}(handle, ham, ovlp, dm, e\_bs)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle            & inout & ELSI handle.\\
\hline
\texttt{ham}    & 1D complex double array & inout & Hamiltonian matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{ovlp}   & 1D complex double array & inout & Overlap matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{dm}     & 1D complex double array & out   & Density matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{e\_bs}  & real double             & out   & Band structure energy.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) When using \api{elsi\_dm\_\{real$\vert$complex\}} with ELPA, libOMM, or EigenExa, the Hamiltonian matrix is destroyed during the computation. The overlap matrix is used to store its Cholesky factorization, which can be reused until the overlap matrix changes.

\section{Customizing ELSI}
\label{sec:setter}
ELSI provides reasonable default values for parameters used in the ELSI interface and the supported solvers. However, no set of default parameters can adequately cover all use cases. Parameters that can be adjusted are described in the following subsections.

\subsection{Customizing the ELSI Interface}
\label{subsec:setter_elsi}
In all the subroutines listed below, the first argument (input and output) is an \texttt{elsi\_handle}. The second argument (input) of each subroutine is the name of parameter to set. Note that logical variables are not used in ELSI API. Integers are used to represent logical, with 0 being false and any positive integer being true.

\begin{longtable}[]{|p{30mm}|p{20mm}|p{15mm}|p{97mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_output}(handle, output\_level)}\\
\multicolumn{4}{l}{\api{elsi\_set\_output\_unit}(handle, output\_unit)}\\
\multicolumn{4}{l}{\api{elsi\_set\_output\_log}(handle, output\_log)}\\
\multicolumn{4}{l}{\api{elsi\_set\_save\_ovlp}(handle, save\_ovlp)}\\
\multicolumn{4}{l}{\api{elsi\_set\_unit\_ovlp}(handle, unit\_ovlp)}\\
\multicolumn{4}{l}{\api{elsi\_set\_zero\_def}(handle, zero\_def)}\\
\multicolumn{4}{l}{\api{elsi\_set\_sparsity\_mask}(handle, sparsity\_mask)}\\
\multicolumn{4}{l}{\api{elsi\_set\_illcond\_check}(handle, illcond\_check)}\\
\multicolumn{4}{l}{\api{elsi\_set\_illcond\_tol}(handle, illcond\_tol)}\\
\multicolumn{4}{l}{\api{elsi\_set\_spin\_degeneracy}(handle, spin\_degeneracy)}\\
\multicolumn{4}{l}{\api{elsi\_set\_energy\_gap}(handle, energy\_gap)}\\
\multicolumn{4}{l}{\api{elsi\_set\_spectrum\_width}(handle, spectrum\_width)}\\
\multicolumn{4}{l}{\api{elsi\_set\_dimensionality}(handle, dimensionality)}\\
\multicolumn{4}{l}{\api{elsi\_set\_mu\_broaden\_scheme}(handle, mu\_broaden\_scheme)}\\
\multicolumn{4}{l}{\api{elsi\_set\_mu\_broaden\_width}(handle, mu\_broaden\_width)}\\
\multicolumn{4}{l}{\api{elsi\_set\_mu\_tol}(handle, mu\_tol)}\\
\multicolumn{4}{l}{\api{elsi\_set\_mu\_mp\_order}(handle, mu\_mp\_order)}\\
\multicolumn{4}{l}{\api{elsi\_set\_write\_unit}(handle, write\_unit)}\\
\multicolumn{4}{l}{\api{elsi\_set\_sing\_check}(handle, sing\_check)}\\
\multicolumn{4}{l}{\api{elsi\_set\_sing\_tol}(handle, sing\_tol)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Default}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{output\_level}       & integer     & 0           & 0: No output. 1: Informative output from ELSI. 2: Informative output from ELSI and the solvers. 3: Informative and debugging output from ELSI and the solvers.\\
\hline
\texttt{output\_unit}        & integer     & 6           & Unit used by ELSI to write out information.\\
\hline
\texttt{output\_log}         & integer     & 0           & If not 0, a separate JSON log file will be written out.\\
\hline
\texttt{save\_ovlp}          & integer     & 0           & If not 0, the overlap matrix will be saved for density matrix extrapolation.\\
\hline
\texttt{unit\_ovlp}          & integer     & 0           & If not 0, the overlap matrix is treated as an identity (unit) matrix. See remark 1.\\
\hline
\texttt{zero\_def}           & real double & $10^{-15}$  & When converting a matrix from dense to sparse format, values below this threshold are discarded.\\
\hline
\texttt{sparsity\_mask}      & integer     & 2           & Which sparsity pattern to use when converting a matrix from dense to sparse format. 0: The union of the sparsity patterns of the Hamiltonian and overlap matrices. 1: The sparsity pattern of the Hamiltonian matrix. 2: The sparsity pattern of the overlap matrix.\\
\hline
\texttt{illcond\_check}      & integer     & 0           & If not 0, the eigenvalues of the overlap matrix will be calculated to check if the overlap matrix is ill-conditioned. See remark 2.\\
\hline
\texttt{illcond\_tol}        & real double & $10^{-5}$   & Eigenfunctions of the overlap matrix with eigenvalues smaller than this threshold will be removed to avoid ill-conditioning. See remark 2.\\
\hline
\texttt{spin\_degeneracy}    & real double & 2.0/n\_spin & Spin degeneracy that controls the maximum number of electrons on a state.\\
\hline
\texttt{energy\_gap}         & real double & 0           & Energy gap. See remark 3.\\
\hline
\texttt{spectrum\_width}     & real double & $10^{3}$    & Width of the eigenspectrum. See remark 3.\\
\hline
\texttt{dimensionality}      & integer     & 3           & Dimensionality (1, 2, or 3) of the physical system being simulated. Only used for automatic solver selection.\\
\hline
\texttt{mu\_broaden\_scheme} & integer     & 0           & Broadening scheme employed to compute the occupation numbers and the Fermi level. 0: Gaussian. 1: Fermi-Dirac. 2: Methfessel-Paxton. 4: Marzari-Vanderbilt. See remark 4.\\
\hline
\texttt{mu\_broaden\_width}  & real double & 0.01        & Broadening width employed to compute the occupation numbers and the Fermi level. See remark 5.\\
\hline
\texttt{mu\_tol}             & real double & $10^{-13}$  & Convergence tolerance (in terms of the absolute error in electron count) of the bisection algorithm employed to compute the occupation numbers and the Fermi level.\\
\hline
\texttt{mu\_mp\_order}       & integer     & 0           & Order of the Methfessel-Paxton broadening scheme. No effect if Methfessel-Paxton is not used.\\
\hline
\texttt{sing\_check}         & integer     & 0           & Deprecated. Use \api{elsi\_set\_illcond\_check} instead.\\
\hline
\texttt{sing\_tol}           & real double & $10^{-5}$   & Deprecated. Use \api{elsi\_set\_illcond\_tol} instead.\\
\hline
\end{longtable}

\textbf{Remarks}

(1) If the overlap matrix is an identity matrix, all settings related to the singularity (ill-conditioning) check are ignored. The \texttt{ovlp} passed into \api{elsi\_\{ev$\vert$dm\}\_\{real$\vert$complex\}\{\_sparse\}} is not referenced.

(2) If the ill-conditioning check is not disabled, in the first iteration of each SCF cycle, all eigenvalues of the overlap matrix are computed. If there is any eigenvalue smaller than \texttt{illcond\_tol}, the matrix is considered to be ill-conditioned.

(3) \texttt{spectrum\_width} and \texttt{energy\_gap} refer to the width and the gap of the eigenspectrum. Simply use the default values if there is no better estimate.

(4) \texttt{mu\_broaden\_scheme}, \texttt{mu\_broaden\_width}, and  \texttt{mu\_tol} are only referenced when using \api{elsi\_dm\_\{real$\vert$complex\}\{\_sparse\}} and an eigensolver. They are ignored when using \api{elsi\_ev\_\{real$\vert$complex\}\{\_sparse\}}, or \api{elsi\_dm\_\{real$\vert$complex\}\{\_sparse\}} with a density matrix solver.

(5) In all supported broadening schemes, there is a term $(\epsilon - E_\text{F})/W$ in the distribution function, where $\epsilon$ is the energy of an eigenstate, and $E_\text{F}$ is the Fermi level. \texttt{broadening\_width} should be $W$ in the same unit of $\epsilon$ and $E_\text{F}$.

\subsection{Customizing the ELPA Solver}
\label{subsec:setter_elpa}
\begin{tabular}[]{|p{30mm}|p{20mm}|p{15mm}|p{97mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_elpa\_solver}(handle, elpa\_solver)}\\
\multicolumn{4}{l}{\api{elsi\_set\_elpa\_n\_single}(handle, elpa\_n\_single)}\\
\multicolumn{4}{l}{\api{elsi\_set\_elpa\_gpu}(handle, elpa\_gpu)}\\
\multicolumn{4}{l}{\api{elsi\_set\_elpa\_autotune}(handle, elpa\_autotune)}\\
\multicolumn{4}{l}{\api{elsi\_set\_elpa\_gpu\_kernels}(handle, elpa\_gpu\_kernels)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Default}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{elpa\_solver}       & integer & 2 & 1: One-stage solver. 2: Two-stage solver (recommended).\\
\hline
\texttt{elpa\_n\_single}    & integer & 0 & Number of SCF steps using single precision ELPA to solve standard eigenproblems. See remark 1.\\
\hline
\texttt{elpa\_gpu}          & integer & 0 & If not 0, enable GPU-acceleration in ELPA. See remark 2.\\
\hline
\texttt{elpa\_autotune}     & integer & 1 & If not 0, enable auto-tuning of runtime parameters in ELPA. Not compatible with \texttt{illcond\_check}.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) \texttt{elpa\_n\_single}: If single precision arithmetic is available in an externally complied ELPA library, it may be enabled by setting \texttt{elpa\_n\_single} to a positive integer, then the standard eigenproblems in the first \texttt{elpa\_n\_single} SCF steps are solved with single precision. The transformations between generalized eigenproblem and the standard form are always performed with double precision. Although this keyword accelerates the solution of standard eigenproblems, the overall SCF convergence may be slower, depending on the physical system and the SCF settings used in the electronic structure code.

(2) \texttt{elpa\_gpu}: If ELPA is compiled with GPU support, GPU acceleration may be enabled by setting \texttt{elpa\_gpu} to a nonzero integer. This keyword is ignored if no GPU support is available.

\subsection{Customizing the libOMM Solver}
\label{subsec:setter_omm}
\begin{tabular}[]{|p{30mm}|p{20mm}|p{15mm}|p{97mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_omm\_flavor}(handle, omm\_flavor)}\\
\multicolumn{4}{l}{\api{elsi\_set\_omm\_n\_elpa}(handle, omm\_n\_elpa)}\\
\multicolumn{4}{l}{\api{elsi\_set\_omm\_tol}(handle, omm\_tol)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Default}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{omm\_flavor}  & integer     & 0          & 0: Direct minimization of a generalized eigenproblem. 2: Cholesky factorization of the overlap matrix transforming the generalized eigenproblem to the standard form.\\
\hline
\texttt{omm\_n\_elpa} & integer     & 6          & Number of SCF steps using ELPA. See remark 1.\\
\hline
\texttt{omm\_tol}     & real double & $10^{-12}$ & Convergence tolerance of orbital minimization. See remark 2.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) \texttt{omm\_n\_elpa}: It has been demonstrated that OMM is optimal at later stages of an SCF cycle where the electronic structure is closer to its local minimum, requiring only one CG iteration to converge the minimization of the OMM energy functional. It is therefore recommended to use ELPA for \texttt{omm\_n\_elpa} SCF steps before switching to libOMM.

(2) \texttt{omm\_tol}: A large minimization tolerance leads to a faster convergence, at the price of a lower accuracy. \texttt{omm\_tol} should be tested and chosen to balance the desired accuracy and computation time.

\subsection{Customizing the PEXSI Solver}
\label{subsec:setter_pexsi}
\begin{tabular}[]{|p{30mm}|p{20mm}|p{15mm}|p{97mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_method}(handle, pexsi\_method)}\\
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_n\_pole}(handle, pexsi\_n\_pole)}\\
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_n\_mu}(handle, pexsi\_n\_mu)}\\
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_np\_per\_pole}(handle, pexsi\_np\_per\_pole)}\\
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_np\_symbo}(handle, pexsi\_np\_symbo)}\\
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_temp}(handle, pexsi\_temp)}\\
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_mu\_min}(handle, pexsi\_mu\_min)}\\
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_mu\_max}(handle, pexsi\_mu\_max)}\\
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_inertia\_tol}(handle, pexsi\_inertia\_tol)}\\
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_gap}(handle, pexsi\_gap)}\\
\multicolumn{4}{l}{\api{elsi\_set\_pexsi\_delta\_e}(handle, pexsi\_delta\_e)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Default}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{pexsi\_method}        & integer     & 3     & 1: Contour integral~\cite{pexsi_lin_2013}. 2: Minimax rational approximation~\cite{pole_moussa_2016}. 3: Adaptive Antoulas-Anderson (AAA)~\cite{aaa_nakatsukasa_2018}. See remark 1.\\
\hline
\texttt{pexsi\_n\_pole}       & integer     & 30    & Number of poles used by PEXSI. See remark 1.\\
\hline
\texttt{pexsi\_n\_mu}         & integer     & 2     & Number of mu points used by PEXSI. See remark 2.\\
\hline
\texttt{pexsi\_np\_per\_pole} & integer     & -     & Number of MPI tasks assigned to one pole. See remark 3.\\
\hline
\texttt{pexsi\_np\_symbo}     & integer     & 1     & Number of MPI tasks for symbolic factorization. See remark 4.\\
\hline
\texttt{pexsi\_temp}          & real double & 0.002 & Electronic temperature. See remark 5.\\
\hline
\texttt{pexsi\_inertia\_tol}  & real double & 0.05  & Stopping criterion of inertia counting. See remark 6.\\
\hline
\texttt{pexsi\_mu\_min}       & real double & -10.0 & Lower bound of mu. See remark 7.\\
\hline
\texttt{pexsi\_mu\_max}       & real double & 10.0  & Upper bound of mu. See remark 7.\\
\hline
\texttt{pexsi\_gap}           & real double & 0.0   & Deprecated. Use \api{elsi\_set\_energy\_gap} instead. See remark 8.\\
\hline
\texttt{pexsi\_delta\_e}      & real double & 10.0  & Deprecated. Use \api{elsi\_set\_spectrum\_width} instead.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) When using the pole expansion method based on contour integral, allowed numbers for \texttt{pexsi\_n\_pole} are: 10, 20, 30, ..., 110, 120. 60 to 100 poles are typically needed to get an accuracy that is comparable with the result obtained from diagonalization. When using the minimax rational approximation or the Adaptive Antoulas-Anderson method, allowed numbers for \texttt{pexsi\_n\_pole} are: 10, 15, 20, ..., 35, 40. 20 to 30 poles are typically needed to get an accuracy that is comparable with the result obtained from diagonalization. PEXSI outputs an error message when it detects an unsupported choice of number of poles.

The electronic entropy can only be computed with the contour integral method and the Adaptive Antoulas-Anderson method. It may be accessed via \api{elsi\_get\_entropy}.

(2) PEXSI determines the chemical potential by performing Fermi operator expansion at several chemical potential values (referred to as ``points'') in an SCF step, then interpolating the results at all points to the final answer. The \texttt{pexsi\_n\_mu} parameter controls the number of chemical potential ``points'' to be evaluated. Two points followed by a simple linear interpolation often yield reasonable results.

(3) \texttt{pexsi\_np\_per\_pole}: PEXSI has three levels of parallelism: the first level evaluates the Fermi operator at all the chemical potential points in parallel; at each chemical potential point, the second level handles the poles in parallel; finally, for each pole, parallel selected inversion is performed as the third level. The value of \texttt{pexsi\_np\_per\_pole} is the number of MPI tasks assigned to one pole at one chemical potential point for the parallel selected inversion. Ideally, the total number of MPI tasks should be \texttt{pexsi\_np\_per\_pole} $\times$ \texttt{pexsi\_n\_mu} $\times$ \texttt{pexsi\_n\_pole}, i.e., all the three levels of parallelism are fully exploited. In case that this is not feasible, PEXSI can also process the poles in serial, whereas all the chemical potential points must be evaluated simultaneously. The user should make sure that the total number of MPI tasks is divisible by the product of the number of MPI tasks per pole and the number of points.

When not using the ``\texttt{PEXSI\_CSC}'' matrix format, \texttt{pexsi\_np\_per\_pole} can be automatically determined to balance the three levels of parallelism in PEXSI. Please note that when using the ``\texttt{PEXSI\_CSC}'' matrix format together with the PEXSI solver, input and output matrices should be distributed among the first \texttt{pexsi\_np\_per\_pole} MPI tasks (not all) in a 1D block distribution. The block size of the distribution must be floor($\text{N}_\text{basis}$/$\text{N}_\text{procs\_per\_pole}$), where floor(x) is the greatest integer less than or equal to x, $\text{N}_\text{basis}$ and $\text{N}_\text{procs\_per\_pole}$ are the number of basis functions and the value of \texttt{pexsi\_np\_per\_pole}, respectively. When using the ``\texttt{PEXSI\_CSC}'' matrix format with the ELPA, libOMM, EigenExa, SLEPc-SIPs, or NTPoly solver, input and output matrices should be distributed across all the MPI tasks in a 1D block distribution. Again, the block size of the distribution must be floor($\text{N}_\text{basis}$/$\text{N}_\text{procs}$).

(4) \texttt{pexsi\_np\_symbo}: Unless there is a memory bottleneck, using 1 MPI task for matrix reordering and symbolic factorization is favorable. When running in serial, the matrix reordering in PT-SCOTCH or ParMETIS introduces a minimal number of ``fill-ins'' to the factorized matrices. Using more MPI tasks introduces more fill-ins. As the matrix reordering and symbolic factorization are performed only once per SCF cycle (with a fixed overlap matrix), using 1 MPI task should not affect the overall timing too much. On the other hand, more fill-ins lead to slower numerical factorization in every SCF step. In addition, the number of MPI tasks used for matrix reordering and symbolic factorization cannot be too large. Otherwise, the symbolic factorization may fail. Therefore, the default number of MPI tasks for symbolic factorization is 1. It is worth testing and increasing this number for large-scale calculations.

(5) \texttt{pexsi\_temp}: This value corresponds to the $k_\text{B} T$ term (not $T$) in the Fermi-Dirac distribution function.

(6) The chemical potential determination in PEXSI relies on an inertia counting step to narrow down the chemical potential searching interval in the first few SCF steps. The inertia counting step is skipped if the difference between \texttt{pexsi\_mu\_min} and \texttt{pexsi\_mu\_max} becomes smaller than \texttt{pexsi\_inertia\_tol}.

(7) PEXSI performs Fermi operator calculations at a number of points within the chemical potential search interval, based on which the chemical potential is determined. In the first SCF iteration of each geometry step, \texttt{pexsi\_mu\_min} and \texttt{pexsi\_mu\_max} should be set to safe values that guarantee the true chemical potential lies in this interval. Then, for the n$^\text{th}$ SCF step, \texttt{pexsi\_mu\_min} should be set to ($\mu_\text{min}^\text{n-1} + \Delta V_\text{min}$), \texttt{pexsi\_mu\_max} should be set to ($\mu_\text{max}^\text{n-1} + \Delta V_\text{max}$). Here, $\mu_\text{min}^\text{n-1}$ and $\mu_\text{max}^\text{n-1}$ are the lower bound and the upper bound of the chemical potential, determined by PEXSI in the (n-1)$^\text{th}$ SCF step. They can be retrieved by calling \api{elsi\_get\_pexsi\_mu\_min} and \api{elsi\_get\_pexsi\_mu\_max}, respectively (see Sec.~\ref{subsec:getter_pexsi}). Suppose the effective potential (Hartree potential, exchange-correlation potential, and external potential) is stored in an array $V$, whose dimension is the number of grid points. From one SCF iteration to the next, $\Delta V$ denotes the potential change, and $\Delta V_\text{min}$ and $\Delta V_\text{max}$ are the minimum and maximum values in the array $\Delta V$, respectively. The whole process is summarized in the pseudo-code below. The (re-)initialization and finalization of ELSI are omitted.

\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
do geometry update
  mu_min = -10.0
  mu_max = 10.0
  delta_V_min = 0.0
  delta_V_max = 0.0

  do SCF cycle
    \tcr{Update Hamiltonian}

    call \tcb{elsi_set_pexsi_mu_min} (eh, mu_min + delta_V_min)
    call \tcb{elsi_set_pexsi_mu_max} (eh, mu_max + delta_V_max)

    call \tcb{elsi_dm_\{real|complex\}} (eh, ham, ovlp, dm, bs_energy)

    call \tcb{elsi_get_pexsi_mu_min} (eh, mu_min)
    call \tcb{elsi_get_pexsi_mu_max} (eh, mu_max)

    \tcr{Update electron density}
    \tcr{Update potential}

    delta_V_min = minval (V_new - V_old)
    delta_V_max = maxval (V_new - V_old)

    \tcr{Check SCF convergence}
  end do
end do
\end{Verbatim}
\end{tcolorbox}

(8) \texttt{pexsi\_gap}: The PEXSI method does not require an energy gap. Use the default value if no knowledge is available.

\subsection{Customizing the EigenExa Solver}
\label{subsec:setter_eigenexa}
\begin{tabular}[]{|p{30mm}|p{20mm}|p{15mm}|p{97mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_eigenexa\_method}(handle, eigenexa\_method)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Default}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{eigenexa\_method} & integer & 2 & 1: Tridiagonalization solver eigen\_s. 2: Pentadiagonalization solver eigen\_sx. The latter is usually faster and more scalable.\\
\hline
\end{tabular}

\subsection{Customizing the SLEPc-SIPs Solver}
\label{subsec:setter_sips}
\begin{tabular}[]{|p{30mm}|p{20mm}|p{15mm}|p{97mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_sips\_ev\_min}(handle, sips\_ev\_min)}\\
\multicolumn{4}{l}{\api{elsi\_set\_sips\_ev\_max}(handle, sips\_ev\_max)}\\
\multicolumn{4}{l}{\api{elsi\_set\_sips\_n\_elpa}(handle, sips\_n\_elpa)}\\
\multicolumn{4}{l}{\api{elsi\_set\_sips\_n\_slice}(handle, sips\_n\_slice)}\\
\multicolumn{4}{l}{\api{elsi\_set\_sips\_interval}(handle, sips\_lower, sips\_upper)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Default}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{sips\_ev\_min}  & real double & -2.0 & Lower bound of eigenspectrum. See remark 1.\\
\hline
\texttt{sips\_ev\_max}  & real double & 2.0  & Upper bound of eigenspectrum. See remark 1.\\
\hline
\texttt{sips\_n\_elpa}  & integer     & 0    & Number of SCF steps using ELPA. See remark 2.\\
\hline
\texttt{sips\_n\_slice} & integer     & 1    & Number of slices. See remark 3.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) \texttt{sips\_ev\_min} and \texttt{sips\_ev\_max}: SLEPc-SIPs relies on an inertia counting step to estimate the lower and upper bounds of the eigenspectrum. Only eigenvalues within this interval, and their associated eigenvectors, are solved. The inertia counting starts from the interval determined by \texttt{sips\_ev\_min} and \texttt{sips\_ev\_max}. This interval may expand or shrink to make sure that it encloses the 1$^\text{st}$ to the \texttt{n\_state}$^\text{th}$ eigenvalues. If a good estimate of the lower or upper bounds of the eigenspectrum is available, it should be set by \api{elsi\_set\_sips\_ev\_min} or \api{elsi\_set\_sips\_ev\_max}.

(2) \texttt{sips\_n\_elpa}: The performance of SLEPc-SIPs mainly depends on the load balance across slices. Optimal performance is expected if the desired eigenvalues are evenly distributed across slices. In an SCF calculation, eigenvalues obtained in one SCF step can be used as an approximate distribution of eigenvalues in the next SCF step. This approximation should become better as the SCF cycle approaches its convergence. Using the direct eigensolver ELPA in the first \texttt{sips\_n\_elpa} SCF steps can circumvent the load imbalance of spectrum slicing in the initial SCF steps.

(3) \texttt{sips\_n\_slice}: SLEPc-SIPs partitions the eigenspectrum into slices and solves the slices in parallel. The number of slices is controlled by \texttt{sips\_n\_slice}. The default value, 1, should always work, but by no means leads to the optimal performance of the solver. There are some general rules to set this parameter. First, as a requirement of the SLEPc library, the total number of MPI tasks must by divisible by \texttt{sips\_n\_slice}. Second, setting \texttt{sips\_n\_slice} to the number of compute nodes usually yields better performance, as the inter-node communication is minimized. The optimal value of \texttt{sips\_n\_slice} depends on the actual problem as well as the hardware.

\subsection{Customizing the NTPoly Solver}
\label{subsec:setter_ntpoly}
\begin{tabular}[]{|p{30mm}|p{20mm}|p{15mm}|p{97mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_ntpoly\_method}(handle, ntpoly\_method)}\\
\multicolumn{4}{l}{\api{elsi\_set\_ntpoly\_filter}(handle, ntpoly\_filter)}\\
\multicolumn{4}{l}{\api{elsi\_set\_ntpoly\_tol}(handle, ntpoly\_tol)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Default}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{ntpoly\_method} & integer     & 2          & 0: Canonical purification~\cite{purification_palser_1998}. 1: 2$^\text{nd}$ order trace resetting purification~\cite{purification_niklasson_2002}. 2: 4$^\text{th}$ order trace resetting purification~\cite{purification_niklasson_2002}. 3: Generalized hole-particle canonical purification~\cite{purification_truflandier_2016}. 1 and 2 are recommended.\\
\hline
\texttt{ntpoly\_filter} & real double & $10^{-15}$ & When performing sparse matrix multiplications, values below this filter are discarded. See remark 1.\\
\hline
\texttt{ntpoly\_tol}    & real double & $10^{-8}$  & Convergence tolerance of purification. See remark 1.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) \texttt{ntpoly\_filter} and \texttt{ntpoly\_tol} control the accuracy and computational cost of a density matrix purification method. Tight choices of \texttt{ntpoly\_filter} and \texttt{ntpoly\_tol}, e.g. the default values here, lead to highly accurate results that are comparable to the results obtained from diagonalization. However, linear scaling can only be achieved with a relatively large \texttt{ntpoly\_filter} such as $10^{-6}$. Note that the purification may not converge if \texttt{ntpoly\_filter} is too large relative to \texttt{ntpoly\_tol}. Setting \texttt{ntpoly\_filter} to be $\le 10^{-3} \times $ \texttt{ntpoly\_tol} is safe in most cases.

\subsection{Customizing the MAGMA Solver}
\label{subsec:setter_magma}
\begin{tabular}[]{|p{30mm}|p{20mm}|p{15mm}|p{97mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_magma\_solver}(handle, magma\_solver)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Default}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{magma\_solver} & integer & 1 & 1: One-stage solver. 2: Two-stage solver.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) MAGMA can use multiple GPUs, controlled by the environment variable \texttt{MAGMA\_NUM\_GPUS}. Refer to the users' guide of MAGMA for more information.

\section{Getting Additional Results from ELSI}
\label{sec:getter}
In Sec.~\ref{sec:ev} and Sec.~\ref{sec:dm}, the interfaces to compute and return the eigensolutions and the density matrices have been introduced. ELSI and the solvers may perform additional calculations whose results are useful at a certain stage of a calculation. One example is the energy-weighted density matrix that is employed to evaluate the Pulay forces during a geometry optimization calculation. The subroutines introduced in the following subsections are used to retrieve such additional results from ELSI.

\subsection{Getting Results from the ELSI Interface}
\label{subsec:getter_elsi}
In all the subroutines listed below, the first argument (input and output) is an \texttt{elsi\_handle}. The second argument (output) of each subroutine is the name of the parameter to get.

\begin{longtable}[]{|p{35mm}|p{40mm}|p{87mm}|}
\multicolumn{3}{l}{\api{elsi\_get\_version}(major, minor, patch)}\\
\multicolumn{3}{l}{\api{elsi\_get\_datestamp}(date\_stamp)}\\
\multicolumn{3}{l}{\api{elsi\_get\_initialized}(handle, handle\_init)}\\
\multicolumn{3}{l}{\api{elsi\_get\_n\_illcond}(handle, n\_illcond)}\\
\multicolumn{3}{l}{\api{elsi\_get\_ovlp\_ev\_min}(handle, ev\_min)}\\
\multicolumn{3}{l}{\api{elsi\_get\_ovlp\_ev\_max}(handle, ev\_max)}\\
\multicolumn{3}{l}{\api{elsi\_get\_mu}(handle, mu)}\\
\multicolumn{3}{l}{\api{elsi\_get\_entropy}(handle, ts)}\\
\multicolumn{3}{l}{\api{elsi\_get\_edm\_real}(handle, edm\_real)}\\
\multicolumn{3}{l}{\api{elsi\_get\_edm\_complex}(handle, edm\_complex)}\\
\multicolumn{3}{l}{\api{elsi\_get\_edm\_real\_sparse}(handle, edm\_real\_sparse)}\\
\multicolumn{3}{l}{\api{elsi\_get\_edm\_complex\_sparse}(handle, edm\_complex\_sparse)}\\
\multicolumn{3}{l}{\api{elsi\_get\_eval}(handle, eval)}\\
\multicolumn{3}{l}{\api{elsi\_get\_evec\_real}(handle, evec\_real)}\\
\multicolumn{3}{l}{\api{elsi\_get\_evec\_complex}(handle, evec\_complex)}\\
\multicolumn{3}{l}{\api{elsi\_get\_occ}(handle, occ)}\\
\multicolumn{3}{l}{\api{elsi\_get\_n\_sing}(handle, n\_sing)}\\
\multicolumn{3}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{major}                & integer                 & Major version number.\\
\hline
\texttt{minor}                & integer                 & Minor version number.\\
\hline
\texttt{patch}                & integer                 & Patch level.\\
\hline
\texttt{date\_stamp}          & integer                 & Date stamp of ELSI (yyyymmdd).\\
\hline
\texttt{handle\_init}         & integer                 & 0 if the ELSI handle has not been initialized; 1 if initialized.\\
\hline
\texttt{n\_illcond}           & integer                 & Number of eigenvalues of the overlap matrix that are smaller than the ill-conditioning tolerance. See Sec.~\ref{subsec:setter_elsi}.\\
\hline
\texttt{ovlp\_ev\_min}        & real double             & Lowest eigenvalue of the overlap matrix. See remark 1.\\
\hline
\texttt{ovlp\_ev\_max}        & real double             & Highest eigenvalue of the overlap matrix. See remark 1.\\
\hline
\texttt{mu}                   & real double             & Chemical potential. See remark 2.\\
\hline
\texttt{ts}                   & real double             & Entropy. See remark 2.\\
\hline
\texttt{edm\_real}            & 2D real double array    & Energy-weighted density matrix in ``\texttt{BLACS\_DENSE}'' format. See remark 3.\\
\hline
\texttt{edm\_complex}         & 2D complex double array & Energy-weighted density matrix in ``\texttt{BLACS\_DENSE}'' format. See remark 3.\\
\hline
\texttt{edm\_real\_sparse}    & 1D real double array    & Energy-weighted density matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format. See remark 3.\\
\hline
\texttt{edm\_complex\_sparse} & 1D complex double array & Energy-weighted density matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format. See remark 3.\\
\hline
\texttt{eval}                 & 1D real double array    & Eigenvalues. See remark 4.\\
\hline
\texttt{evec\_real}           & 2D real double array    & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format. See remark 4.\\
\hline
\texttt{evec\_complex}        & 2D complex double array & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format. See remark 4.\\
\hline
\texttt{occ}                  & 1D real double array    & Occupation numbers. See remark 4.\\
\hline
\texttt{n\_sing}              & integer                 & Deprecated. Use \api{elsi\_get\_n\_illcond} instead.\\
\hline
\end{longtable}

\textbf{Remarks}

(1) Ill-conditioning check of the overlap matrix is enabled by default when ELPA is the chosen solver. It may be disabled by calling \api{elsi\_set\_illcond\_check}, and is automatically disabled when the chosen solver is not ELPA. \texttt{ovlp\_ev\_min} and \texttt{ovlp\_ev\_max} are computed only if ill-conditioning check is enabled. Otherwise the return value may be zero.

(2) The chemical potential is available only if \api{elsi\_dm\_\{real$\vert$complex\}\{\_sparse\}} has been called, with ELPA, PEXSI, SLEPc-SIPs, EigenExa, or NTPoly being the chosen solver. The entropy is available only if \api{elsi\_dm\_\{real$\vert$complex\}\{\_sparse\}} has been called with ELPA, PEXSI (see also \api{elsi\_set\_pexsi\_method}), SLEPc-SIPs, or EigenExa being the chosen solver. ELSI may return zero when the chemical potential or the entropy is not available.

(3) In general, the energy-weighted density matrix is only needed in a late stage of an SCF cycle to evaluate forces. It is, therefore, not calculated when any of the density matrix solver interface is called. When the energy-weighted density matrix is actually needed, it can be requested by calling \api{elsi\_get\_edm\_\{real$\vert$complex\}\{\_sparse\}}. These subroutines have the requirement that the corresponding \api{elsi\_dm} subroutine must have been invoked. For instance, \api{elsi\_get\_edm\_real\_sparse} only makes sense if \api{elsi\_dm\_real\_sparse} has been successfully executed.

(4) When using \api{elsi\_dm\_\{real$\vert$complex\}\{\_sparse\}} with an eigensolver, ELSI internally computes and stores the eigenvalues, eigenvectors, and occupation numbers. These quantities may be retrieved by calling \api{elsi\_get\_eval}, \api{elsi\_get\_evec\_\{real$\vert$complex\}}, and \api{elsi\_get\_occ}. The dimension of \texttt{eval} and \texttt{occ} should be equal to the value of \texttt{n\_states} set in \api{elsi\_init}. Even with \api{elsi\_dm\_\{real$\vert$complex\}\_sparse}, the eigenvectors are returned in a dense format (``\texttt{BLACS\_DENSE}''), as they are in general not sparse. The size of \texttt{evec\_\{real$\vert$complex\}} should always correspond to a global array of size \texttt{n\_basis} by \texttt{n\_basis}, regardless of the value of \texttt{n\_states}.

\subsection{Getting Results from the PEXSI Solver}
\label{subsec:getter_pexsi}
\begin{tabular}[]{|p{30mm}|p{20mm}|p{112mm}|}
\multicolumn{3}{l}{\api{elsi\_get\_pexsi\_mu\_min}(handle, pexsi\_mu\_min)}\\
\multicolumn{3}{l}{\api{elsi\_get\_pexsi\_mu\_max}(handle, pexsi\_mu\_max)}\\
\multicolumn{3}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{pexsi\_mu\_min} & real double & Minimum value of mu. See remark 1.\\
\hline
\texttt{pexsi\_mu\_max} & real double & Maximum value of mu. See remark 1.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) Please refer to Sec.~\ref{subsec:setter_pexsi} for the chemical potential determination algorithm in PEXSI and ELSI.

\subsection{Extrapolation of Wavefunctions and Density Matrices}
\label{subsec:extrapolation}
In geometry optimization and molecular dynamics calculations, the initial guess of the electron density in the (n+1)$^\text{th}$ geometry step can be constructed from the wavefunctions or density matrix calculated in the n$^\text{th}$ geometry step. However, due to the movement of atoms and localized basis functions around them, wavefunctions obtained in the n$^\text{th}$ geometry step are no longer orthonormalized in the (n+1)$^\text{th}$ geometry step. \api{elsi\_orthonormalize\_ev\_\{real$\vert$complex\}\{\_sparse\}} orthonormalizes eigenvectors (coefficients of wavefunctions) in the n$^\text{th}$ geometry step with respect to the overlap matrix in the (n+1)$^\text{th}$ geometry step with a Gram-Schmidt algorithm.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_orthonormalize\_ev\_real}(handle, ovlp, evec)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle         & inout & ELSI handle.\\
\hline
\texttt{ovlp}   & 2D real double array & in    & Overlap matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{evec}   & 2D real double array & inout & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_orthonormalize\_ev\_complex}(handle, ovlp, evec)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle            & inout & ELSI handle.\\
\hline
\texttt{ovlp}   & 2D complex double array & in    & Overlap matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{evec}   & 2D complex double array & inout & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_orthonormalize\_ev\_real\_sparse}(handle, ovlp, evec)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle         & inout & ELSI handle.\\
\hline
\texttt{ovlp}   & 1D real double array & in    & Overlap matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{evec}   & 2D real double array & inout & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format. See remark 1.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_orthonormalize\_ev\_complex\_sparse}(handle, ovlp, evec)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle            & inout & ELSI handle.\\
\hline
\texttt{ovlp}   & 1D complex double array & in    & Overlap matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{evec}   & 2D complex double array & inout & Eigenvectors in ``\texttt{BLACS\_DENSE}'' format. See remark 1.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) Even when using \api{elsi\_orthonormalize\_ev\_\{real$\vert$complex\}\_sparse}, the eigenvectors are still stored in a dense format (``\texttt{BLACS\_DENSE}''), as they are in general not sparse.

\api{elsi\_extrapolate\_dm\_\{real$\vert$complex\}\{\_sparse\}} extrapolates density matrix in the n$^\text{th}$ geometry step to the overlap matrix in the (n+1)$^\text{th}$ geometry step. \api{elsi\_set\_save\_ovlp} must have been called to store the relevant matrices in the n$^\text{th}$ geometry step within ELSI.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_extrapolate\_dm\_real}(handle, ovlp, dm)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle         & inout & ELSI handle.\\
\hline
\texttt{ovlp}   & 2D real double array & in    & New overlap matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{dm}     & 2D real double array & out   & New density matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_extrapolate\_dm\_complex}(handle, ovlp, dm)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle            & inout & ELSI handle.\\
\hline
\texttt{ovlp}   & 2D complex double array & in    & New overlap matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\texttt{dm}     & 2D complex double array & out   & New density matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_extrapolate\_dm\_real\_sparse}(handle, ovlp, dm)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle         & inout & ELSI handle.\\
\hline
\texttt{ovlp}   & 1D real double array & in    & New overlap matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{dm}     & 1D real double array & out   & New density matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_extrapolate\_dm\_complex\_sparse}(handle, ovlp, dm)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_handle            & inout & ELSI handle.\\
\hline
\texttt{ovlp}   & 1D complex double array & in    & New overlap matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\texttt{dm}     & 1D complex double array & out   & New density matrix in ``\texttt{PEXSI\_CSC}'', ``\texttt{SIESTA\_CSC}'', or ``\texttt{GENERIC\_COO}'' sparse format.\\
\hline
\end{tabular}

\section{Parallel Matrix I/O}
\label{sec:rw}
The ELSI interface is able to read and write distributed matrices in parallel. There exist a number of libraries for high-performance parallel I/O that are particularly capable of reading and writing a large amount of data with hierarchical structures and complex metadata. However, the data structure in ELSI is simply arrays that represent matrices, with a few integers to define the dimension of the matrices. In order to circumvent the development and performance overhead associated with a high level I/O library, ELSI directly uses the parallel I/O functionality defined in the MPI standard.

Writing the distributed matrices into $\text{N}_\text{procs}$ separate files, where $\text{N}_\text{procs}$ is the number of MPI tasks, is not preferred, as manipulating a large number of files would be difficult. The implementation of matrix I/O in ELSI adopts collective MPI I/O routines to write data to (read data from) a single binary file, as if the data was gathered onto a single MPI task then written to one file (read from one file by one MPI task then scattered to all tasks). The optimal I/O performance, both with MPI I/O and in general, is achieved by making large and contiguous requests to access the file system. Therefore, ELSI always redistributes the matrices to a 1D block distribution before writing it to file. This guarantees that each MPI task writes a contiguous chunk of data to a contiguous piece of file. Similarly, matrices read from file are in a 1D block distribution, and can be redistributed automatically if needed. ELSI always stores matrices in a sparse CSC format. The conversion between dense and sparse formats is handled automatically.

\subsection{Setting Up Matrix I/O}
\label{subsec:rw_init}
An \texttt{elsi\_rw\_handle} must be initialized via the \api{elsi\_init\_rw} subroutine before any other matrix I/O subroutine may be called. This \texttt{elsi\_rw\_handle} must be passed to all other matrix I/O subroutine calls.

\begin{tabular}[]{|p{25mm}|p{25mm}|p{10mm}|p{102mm}|}
\multicolumn{4}{l}{\api{elsi\_init\_rw}(handle, task, parallel\_mode, n\_basis, n\_electron)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}         & elsi\_rw\_handle & out & ELSI matrix I/O handle.\\
\hline
\texttt{task}           & integer          & in  & 0: \texttt{READ\_MATRIX}. 1: \texttt{WRITE\_MATRIX}.\\
\hline
\texttt{parallel\_mode} & integer          & in  & 0: \texttt{SINGLE\_PROC}. 1: \texttt{MULTI\_PROC}. See \api{elsi\_init}.\\
\hline
\texttt{n\_electron}    & real double      & in  & Number of electrons. See remark 1.\\
\hline
\texttt{n\_basis}       & integer          & in  & Number of basis functions, i.e. global size of matrix.\\
\hline
\end{tabular}

\textbf{Remarks}

(1) \texttt{n\_electron}: Matrices written out with ELSI matrix I/O are usually from actual electronic structure calculations. Having the number of electrons available makes the matrix file useful for testing density matrix solvers such as PEXSI. Therefore, it is recommended to set the correct number of electrons when initializing an matrix I/O handle, although setting it to an arbitrary number does not affect the matrix I/O operation.

(2) \texttt{n\_basis}: This can be set to an arbitrary value if \texttt{task} is ``\texttt{READ\_MATRIX}''. Its value is read from file when calling \api{elsi\_read\_mat\_dim} or \api{elsi\_read\_mat\_dim\_sparse}.

The MPI communicator which encloses the MPI tasks to perform the matrix I/O operation needs to be passed into ELSI via the \api{elsi\_set\_rw\_mpi} subroutine.

\begin{tabular}[]{|p{25mm}|p{25mm}|p{10mm}|p{102mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_rw\_mpi}(handle, comm)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_rw\_handle & inout & ELSI matrix I/O handle.\\
\hline
\texttt{comm}   & integer          & in    & MPI communicator.\\
\hline
\end{tabular}

When reading or writing a dense matrix, BLACS parameters are passed into ELSI via the \api{elsi\_set\_rw\_blacs} subroutine.

\begin{tabular}[]{|p{25mm}|p{25mm}|p{10mm}|p{102mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_rw\_blacs}(handle, blacs\_ctxt, block\_size)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}      & elsi\_rw\_handle & inout & ELSI matrix I/O handle.\\
\hline
\texttt{blacs\_ctxt} & integer          & in    & BLACS context.\\
\hline
\texttt{block\_size} & integer          & in    & Block size of the 2D block-cyclic distribution, specifying both row and column directions.\\
\hline
\end{tabular}

When writing a sparse matrix, its dimensions are passed into ELSI via the \api{elsi\_set\_rw\_csc} subroutine. The only sparse matrix format currently supported by ELSI matrix I/O is the ``\texttt{PEXSI\_CSC}'' format. When reading a sparse matrix, there is no need to call this subroutine. The relevant parameters are read from file when calling \api{elsi\_read\_mat\_dim} or \api{elsi\_read\_mat\_dim\_sparse}.

\begin{tabular}[]{|p{25mm}|p{25mm}|p{10mm}|p{102mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_rw\_csc}(handle, global\_nnz, local\_nnz, local\_col)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}      & elsi\_rw\_handle & inout & ELSI matrix I/O handle.\\
\hline
\texttt{global\_nnz} & integer          & in    & Global number of non-zeros.\\
\hline
\texttt{local\_nnz}  & integer          & in    & Local number of non-zeros.\\
\hline
\texttt{local\_col}  & integer          & in    & Local number of matrix columns.\\
\hline
\end{tabular}

When a matrix I/O instance is no longer needed, its associated handle should be cleaned up by calling \api{elsi\_finalize\_rw}.

\begin{tabular}[]{|p{25mm}|p{25mm}|p{10mm}|p{102mm}|}
\multicolumn{4}{l}{\api{elsi\_finalize\_rw}(handle)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_rw\_handle & inout & ELSI matrix I/O handle.\\
\hline
\end{tabular}

\subsection{Writing Matrices}
\label{subsec:rw_write}
\api{elsi\_write\_mat\_\{real$\vert$complex\}} writes a dense matrix to file. Before writing a dense matrix, MPI and BLACS should be set up properly using \api{elsi\_set\_rw\_mpi} and \api{elsi\_set\_rw\_blacs}.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_write\_mat\_real}(handle, filename, mat)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}   & elsi\_rw\_handle     & in & ELSI matrix I/O handle.\\
\hline
\texttt{filename} & string               & in & Name of file to write.\\
\hline
\texttt{mat}      & 2D real double array & in & Matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_write\_mat\_complex}(handle, filename, mat)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}   & elsi\_rw\_handle        & in & ELSI matrix I/O handle.\\
\hline
\texttt{filename} & string                  & in & Name of file to write.\\
\hline
\texttt{mat}      & 2D complex double array & in & Matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\end{tabular}

\api{elsi\_write\_mat\_\{real$\vert$complex\}\_sparse} writes a sparse matrix to file. Before writing a sparse matrix, MPI and CSC matrix format should be set up properly using \api{elsi\_set\_rw\_mpi} and \api{elsi\_set\_rw\_csc}.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_write\_mat\_real\_sparse}(handle, filename, row\_idx, col\_ptr, mat)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}   & elsi\_rw\_handle     & in & ELSI matrix I/O handle.\\
\hline
\texttt{filename} & string               & in & Name of file to write.\\
\hline
\texttt{row\_idx} & 1D integer array     & in & Local row index array.\\
\hline
\texttt{col\_ptr} & 1D integer array     & in & Local column pointer array.\\
\hline
\texttt{mat}      & 1D real double array & in & Matrix in ``\texttt{PEXSI\_CSC}'' format.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_write\_mat\_complex\_sparse}(handle, filename, row\_idx, col\_ptr, mat)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}   & elsi\_rw\_handle        & in & ELSI matrix I/O handle.\\
\hline
\texttt{filename} & string                  & in & Name of file to write.\\
\hline
\texttt{row\_idx} & 1D integer array        & in & Local row index array.\\
\hline
\texttt{col\_ptr} & 1D integer array        & in & Local column pointer array.\\
\hline
\texttt{mat}      & 1D complex double array & in & Matrix in ``\texttt{PEXSI\_CSC}'' format.\\
\hline
\end{tabular}

When writing a dense matrix to file, values smaller than a predefined threshold are discarded. The default value of this threshold is $10^{-15}$. It can be overridden via \api{elsi\_set\_rw\_zero\_def}.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_rw\_zero\_def}(handle, zero\_def)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}    & elsi\_rw\_handle & inout & ELSI matrix I/O handle.\\
\hline
\texttt{zero\_def} & real double      & in    & When writing a dense matrix to file, values below this threshold are discarded.\\
\hline
\end{tabular}

An array of eight user-defined integers can be optionally set up via \api{elsi\_set\_rw\_header}. This array is attached to the matrix file written out by \api{elsi\_write\_mat\_\{real$\vert$complex\}\{\_sparse\}}. When reading a matrix file, this array may be retrieved via \api{elsi\_get\_rw\_header}.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_set\_rw\_header}(handle, header)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_rw\_handle & inout & ELSI matrix I/O handle.\\
\hline
\texttt{header} & 1D integer array & in    & An array of eight integers.\\
\hline
\end{tabular}

\subsection{Reading Matrices}
\label{subsec:rw_read}
\api{elsi\_real\_mat\_\{real$\vert$complex\}\{\_sparse\}} reads a dense or sparse matrix from file. While writing a matrix to file can be done in one step, it is easier to read a matrix from file in two steps, i.e., first read the dimension of the matrix and allocate memory accordingly, then read the actual data of the matrix.

Before reading a dense matrix, MPI and BLACS should be set up properly using \api{elsi\_set\_rw\_mpi} and \api{elsi\_set\_rw\_blacs}. \api{elsi\_read\_mat\_dim} is used to read the dimension of a matrix, including the number of electrons in the physical system (for testing purpose), the global size of the matrix, and the local size of the matrix. Memory needs to be allocated according to the return values of \texttt{local\_row} and \texttt{local\_col}. Then \api{elsi\_read\_mat\_\{real$\vert$complex\}} may be called to read a real or complex matrix, respectively.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_read\_mat\_dim}(handle, filename, n\_electron, n\_basis, local\_row, local\_col)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}      & elsi\_rw\_handle & inout & ELSI matrix I/O handle.\\
\hline
\texttt{filename}    & string           & in    & Name of file to read.\\
\hline
\texttt{n\_electron} & real double      & out   & Number of electrons.\\
\hline
\texttt{n\_basis}    & integer          & out   & Number of basis functions, i.e. global size of matrix.\\
\hline
\texttt{local\_row}  & integer          & out   & Local number of matrix rows.\\
\hline
\texttt{local\_col}  & integer          & out   & Local number of matrix columns.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_read\_mat\_real}(handle, filename, mat)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}   & elsi\_rw\_handle     & inout & ELSI matrix I/O handle.\\
\hline
\texttt{filename} & string               & in    & Name of file to read.\\
\hline
\texttt{mat}      & 2D real double array & out   & Matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_read\_mat\_complex}(handle, filename, mat)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}   & elsi\_rw\_handle        & inout & ELSI matrix I/O handle.\\
\hline
\texttt{filename} & string                  & in    & Name of file to read.\\
\hline
\texttt{mat}      & 2D complex double array & out   & Matrix in ``\texttt{BLACS\_DENSE}'' format.\\
\hline
\end{tabular}

Before reading a sparse matrix, MPI should be set up properly using \api{elsi\_set\_rw\_mpi}. \api{elsi\_read\_mat\_dim\_sparse} is used to read the dimension of a matrix, including the number of electrons in the physical system (for testing purpose), the global size of the matrix, and the local size of the matrix. Memory needs to be allocated according to the return values of \texttt{local\_nnz} and \texttt{local\_col}. Then \api{elsi\_read\_mat\_\{real$\vert$complex\}\_sparse} may be called to read a real or complex matrix, respectively.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_read\_mat\_dim\_sparse}(handle, filename, n\_electron, n\_basis, global\_nnz, local\_nnz, local\_col)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}      & elsi\_rw\_handle & inout & ELSI matrix I/O handle.\\
\hline
\texttt{filename}    & string           & in    & Name of file to read.\\
\hline
\texttt{n\_electron} & real double      & out   & Number of electrons.\\
\hline
\texttt{n\_basis}    & integer          & out   & Number of basis functions, i.e. global size of matrix.\\
\hline
\texttt{global\_nnz} & integer          & out   & Global number of non-zeros.\\
\hline
\texttt{local\_nnz}  & integer          & out   & Local number of non-zeros.\\
\hline
\texttt{local\_col}  & integer          & out   & Local number of matrix columns.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_read\_mat\_real\_sparse}(handle, filename, row\_idx, col\_ptr, mat)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}   & elsi\_rw\_handle     & inout & ELSI matrix I/O handle.\\
\hline
\texttt{filename} & string               & in    & Name of file to read.\\
\hline
\texttt{row\_idx} & 1D integer array     & out   & Local row index array.\\
\hline
\texttt{col\_ptr} & 1D integer array     & out   & Local column pointer array.\\
\hline
\texttt{mat}      & 1D real double array & out   & Matrix in ``\texttt{PEXSI\_CSC}'' format.\\
\hline
\end{tabular}

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_read\_mat\_complex\_sparse}(handle, filename, row\_idx, col\_ptr, mat)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle}   & elsi\_rw\_handle        & inout & ELSI matrix I/O handle.\\
\hline
\texttt{filename} & string                  & in    & Name of file to read.\\
\hline
\texttt{row\_idx} & 1D integer array        & out   & Local row index array.\\
\hline
\texttt{col\_ptr} & 1D integer array        & out   & Local column pointer array.\\
\hline
\texttt{mat}      & 1D complex double array & out   & Matrix in ``\texttt{PEXSI\_CSC}'' format.\\
\hline
\end{tabular}

An array of eight user-defined integers can be optionally set up via \api{elsi\_set\_rw\_header}. This array is attached to the matrix file written out by \api{elsi\_write\_mat\_\{real$\vert$complex\}\{\_sparse\}}. When reading a matrix file, this array may be retrieved via \api{elsi\_get\_rw\_header}.

\begin{tabular}[]{|p{20mm}|p{40mm}|p{10mm}|p{92mm}|}
\multicolumn{4}{l}{\api{elsi\_get\_rw\_header}(handle, header)}\\
\multicolumn{4}{l}{}\\
\hline
\multicolumn{1}{|l|}{\textbf{Argument}} & \multicolumn{1}{l|}{\textbf{Data Type}} & \multicolumn{1}{l|}{\textbf{in/out}} & \multicolumn{1}{l|}{\textbf{Explanation}}\\
\hline
\texttt{handle} & elsi\_rw\_handle  & inout & ELSI matrix I/O handle.\\
\hline
\texttt{header} & 1D integer array  & out   & An array of eight integers.\\
\hline
\end{tabular}

\section{C/C++ Interface}
\label{sec:c}
ELSI is written in Fortran. A C interface around the core Fortran code is provided, which can be called from a C or C++ program. Each C wrapper function corresponds to a Fortran subroutine, where we have prefixed the original Fortran subroutine name with \texttt{c\_} for clarity and consistency. Argument lists are identical to the associated native Fortran subroutine. For the complete definition of the C interface, the user is encouraged to look at the \texttt{elsi.h} header file directly.

\section{Example Pseudo-Code}
\label{sec:example}
Typical workflow of ELSI within an electronic structure code is demonstrated by the following pseudo-code. In the ``\texttt{test}'' directory of the ELSI package, there are also examples that showcase the usage of ELSI in C and Fortran.

\subsection*{2D Block-Cyclic Distributed Dense Matrix + ELSI Eigensolver Interface}
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
\tcr{SCF initialize}

call \tcb{elsi_init} (eh, ELPA, MULTI_PROC, BLACS_DENSE, n_basis, n_electron, n_state)
call \tcb{elsi_set_mpi} (eh, mpi_comm)
call \tcb{elsi_set_blacs} (eh, blacs_ctxt, block_size)

do SCF cycle
  \tcr{Update Hamiltonian}

  call \tcb{elsi_ev_\{real|complex\}} (eh, ham, ovlp, eval, evec)

  \tcr{Update electron density}
  \tcr{Check SCF convergence}
end do

call \tcb{elsi_finalize} (eh)
\end{Verbatim}
\end{tcolorbox}

\subsection*{1D Block-Cyclic Distributed CSC Sparse Matrix + ELSI Eigensolver Interface}
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
\tcr{SCF initialize}

call \tcb{elsi_init} (eh, ELPA, MULTI_PROC, SIESTA_CSC, n_basis, n_electron, n_state)
call \tcb{elsi_set_mpi} (eh, mpi_comm)
call \tcb{elsi_set_blacs} (eh, blacs_ctxt, block_size)
call \tcb{elsi_set_csc} (eh, global_nnz, local_nnz, local_col, row_idx, col_ptr)
call \tcb{elsi_set_csc_blk} (eh, block_size_csc)

do SCF cycle
  \tcr{Update Hamiltonian}

  call \tcb{elsi_ev_\{real|complex\}_sparse} (eh, ham, ovlp, eval, evec)

  \tcr{Update electron density}
  \tcr{Check SCF convergence}
end do

call \tcb{elsi_finalize} (eh)
\end{Verbatim}
\end{tcolorbox}

\textbf{Remarks}

(1) Eigenvectors are returned in the ``\texttt{BLACS\_DENSE}'' format, which is required to be properly set up.

\subsection*{Arbitrarily Distributed COO Sparse Matrix + ELSI Eigensolver Interface}
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
\tcr{SCF initialize}

call \tcb{elsi_init} (eh, ELPA, MULTI_PROC, GENERIC_COO, n_basis, n_electron, n_state)
call \tcb{elsi_set_mpi} (eh, mpi_comm)
call \tcb{elsi_set_blacs} (eh, blacs_ctxt, block_size)
call \tcb{elsi_set_coo} (eh, global_nnz, local_nnz, row_idx, col_idx)

do SCF cycle
  \tcr{Update Hamiltonian}

  call \tcb{elsi_ev_\{real|complex\}_sparse} (eh, ham, ovlp, eval, evec)

  \tcr{Update electron density}
  \tcr{Check SCF convergence}
end do

call \tcb{elsi_finalize} (eh)
\end{Verbatim}
\end{tcolorbox}

\textbf{Remarks}

(1) Eigenvectors are returned in the ``\texttt{BLACS\_DENSE}'' format, which is required to be properly set up.

\subsection*{2D Block-Cyclic Distributed Dense Matrix + ELSI Density Matrix Interface}
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
\tcr{SCF initialize}

call \tcb{elsi_init} (eh, OMM, MULTI_PROC, BLACS_DENSE, n_basis, n_electron, n_state)
call \tcb{elsi_set_mpi} (eh, mpi_comm)
call \tcb{elsi_set_blacs} (eh, blacs_ctxt, block_size)

do SCF cycle
  \tcr{Update Hamiltonian}

  call \tcb{elsi_dm_\{real|complex\}} (eh, ham, ovlp, dm, bs_energy)

  \tcr{Update electron density}
  \tcr{Check SCF convergence}
end do

call \tcb{elsi_finalize} (eh)
\end{Verbatim}
\end{tcolorbox}

\subsection*{1D Block-Cyclic Distributed CSC Sparse Matrix + ELSI Density Matrix Interface}
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
\tcr{SCF initialize}

call \tcb{elsi_init} (eh, PEXSI, MULTI_PROC, SIESTA_CSC, n_basis, n_electron, n_state)
call \tcb{elsi_set_mpi} (eh, mpi_comm)
call \tcb{elsi_set_csc} (eh, global_nnz, local_nnz, local_col, row_idx, col_ptr)
call \tcb{elsi_set_csc_blk} (eh, block_size)

do SCF cycle
  \tcr{Update Hamiltonian}

  call \tcb{elsi_dm_\{real|complex\}_sparse} (eh, ham, ovlp, dm, bs_energy)
  call \tcb{elsi_get_edm_\{real|complex\}_sparse} (eh, edm)

  \tcr{Update electron density}
  \tcr{Check SCF convergence}
end do

call \tcb{elsi_finalize} (eh)
\end{Verbatim}
\end{tcolorbox}

\textbf{Remarks}

(1) Refer to Sec.~\ref{subsec:setter_pexsi} for the chemical potential determination algorithm in PEXSI.

\subsection*{Arbitrarily Distributed COO Sparse Matrix + ELSI Density Matrix Interface}
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
\tcr{SCF initialize}

call \tcb{elsi_init} (eh, PEXSI, MULTI_PROC, GENERIC_COO, n_basis, n_electron, n_state)
call \tcb{elsi_set_mpi} (eh, mpi_comm)
call \tcb{elsi_set_coo} (eh, global_nnz, local_nnz, row_idx, col_idx)

do SCF cycle
  \tcr{Update Hamiltonian}

  call \tcb{elsi_dm_\{real|complex\}_sparse} (eh, ham, ovlp, dm, bs_energy)
  call \tcb{elsi_get_edm_\{real|complex\}_sparse} (eh, edm)

  \tcr{Update electron density}
  \tcr{Check SCF convergence}
end do

call \tcb{elsi_finalize} (eh)
\end{Verbatim}
\end{tcolorbox}

\textbf{Remarks}

(1) Refer to Sec.~\ref{subsec:setter_pexsi} for the chemical potential determination algorithm in PEXSI.

\subsection*{Multiple $k$-points Calculations}
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
\tcr{SCF initialize}

call \tcb{elsi_init} (eh, NTPOLY, MULTI_PROC, BLACS_DENSE, n_basis, n_electron, n_state)
call \tcb{elsi_set_mpi} (eh, mpi_comm)
call \tcb{elsi_set_blacs} (eh, blacs_ctxt, block_size)
call \tcb{elsi_set_kpoint} (eh, n_kpt, i_kpt, i_wt)
call \tcb{elsi_set_mpi_global} (eh, mpi_comm_global)

do SCF cycle
  \tcr{Update Hamiltonian}

  call \tcb{elsi_dm_\{real|complex\}} (eh, ham, ovlp, dm, bs_energy)
  call \tcb{elsi_get_edm_\{real|complex\}} (eh, edm)

  \tcr{Update electron density}
  \tcr{Check SCF convergence}
end do

call \tcb{elsi_finalize} (eh)
\end{Verbatim}
\end{tcolorbox}

\textbf{Remarks}

(1) When there are multiple $k$-points, there is no change in the way ELSI solver interfaces are called.

(2) The electronic structure code needs to assemble the real-space density from the density matrices returned for the $k$-points. The returned band structure energy, however, is already summed over all $k$-points with respect to the weight of each $k$-point. Refer to Sec.~\ref{subsec:setup_kpt} for more information.

(3) Spin-polarized calculations may be set up similarly.

\subsection*{Geometry Relaxation Calculations}
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
\tcr{SCF initialize}

call \tcb{elsi_init} (eh, ...)
call \tcb{elsi_set_*} (eh, ...)

do geometry
  do SCF cycle
    \tcr{Update Hamiltonian}

    call \tcb{elsi_\{ev|dm\}_\{real|complex\}} (eh, ham, ovlp, ...)

    \tcr{Update electron density}
    \tcr{Check SCF convergence}
  end do

  \tcr{Update geometry (overlap)}

  call \tcb{elsi_reinit} (eh)
end do

call \tcb{elsi_finalize} (eh)
\end{Verbatim}
\end{tcolorbox}

\subsection*{Standard Eigenproblem}
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
call \tcb{elsi_init} (eh, ELPA, MULTI_PROC, BLACS_DENSE, n_basis, n_electron, n_state)
call \tcb{elsi_set_mpi} (eh, mpi_comm)
call \tcb{elsi_set_blacs} (eh, blacs_ctxt, block_size)
call \tcb{elsi_set_unit_overlap} (eh, 1)
call \tcb{elsi_ev_\{real|complex\}} (eh, mat, dummy, eval, evec)
call \tcb{elsi_finalize} (eh)
\end{Verbatim}
\end{tcolorbox}

\subsection*{Bethe-Salpeter Eigenproblem}
\begin{tcolorbox}
\begin{Verbatim}[commandchars=\\\{\}]
call \tcb{elsi_init} (eh, BSEPACK, MULTI_PROC, BLACS_DENSE, n_basis, n_electron, n_state)
call \tcb{elsi_set_mpi} (eh, mpi_comm)
call \tcb{elsi_set_blacs} (eh, blacs_ctxt, block_size)
call \tcb{elsi_bse_\{real|complex\}} (eh, A, B, eval, evec)
call \tcb{elsi_finalize} (eh)
\end{Verbatim}
\end{tcolorbox}

% Reference
\bibliographystyle{elsarticle-num}
\bibliography{elsi_manual}

\chapter*{License and Copyright}
ELSI interface software is licensed under the 3-clause BSD license:

\begin{tcolorbox}
\begin{Verbatim}
Copyright (c) 2015-2020, the ELSI team.
All rights reserved.

Redistribution and use in source and binary forms, with or without modification, are permitted
provided that the following conditions are met:

1) Redistributions of source code must retain the above copyright notice, this list of
   conditions and the following disclaimer.

2) Redistributions in binary form must reproduce the above copyright notice, this list of
   conditions and the following disclaimer in the documentation and/or other materials
   provided with the distribution.

3) Neither the name of the "ELectronic Structure Infrastructure (ELSI)" project nor the names
   of its contributors may be used to endorse or promote products derived from this software
   without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR
IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY
AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL COPYRIGHT HOLDER BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA,
OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
\end{Verbatim}
\end{tcolorbox}

The source code of ELPA 2020.05.001 (LGPL3), libOMM 1.0.0 (BSD2), NTPoly 2.4.0 (MIT), PEXSI 1.2.0 (BSD3), PT-SCOTCH 6.0.0 (CeCILL-C), SuperLU\_DIST 6.2.0 (BSD3), and BSEPACK 0.1 (BSD3) are redistributed through this version of ELSI. Individual license of each library can be found in the corresponding subfolder.

\end{document}
